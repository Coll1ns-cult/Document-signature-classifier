{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ual6VcSkxO2G",
        "outputId": "205244d0-3417-4876-8f87-2343381e1cfe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0CwdFQKLav9o"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import pandas as pd \n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from skimage import io\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Sampler\n",
        "from torch.nn import Module\n",
        "import torch.optim \n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from time import sleep"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NShotTaskSampler(Sampler):\n",
        "    def __init__(self,\n",
        "                 dataset: torch.utils.data.Dataset,\n",
        "                 episodes_per_epoch: int = None,\n",
        "                 n: int = None,\n",
        "                 k: int = None,\n",
        "                 q: int = None,\n",
        "                 num_tasks: int = 1\n",
        "                #  fixed_tasks: List[Iterable[int]] = None\n",
        "    ):\n",
        "        \"\"\"PyTorch Sampler subclass that generates batches of n-shot, k-way, q-query tasks.\n",
        "\n",
        "        Each n-shot task contains a \"support set\" of `k` sets of `n` samples and a \"query set\" of `k` sets\n",
        "        of `q` samples. The support set and the query set are all grouped into one Tensor such that the first n * k\n",
        "        samples are from the support set while the remaining q * k samples are from the query set.\n",
        "\n",
        "        The support and query sets are sampled such that they are disjoint i.e. do not contain overlapping samples.\n",
        "\n",
        "        # Arguments\n",
        "            dataset: Instance of torch.utils.data.Dataset from which to draw samples\n",
        "            episodes_per_epoch: Arbitrary number of batches of n-shot tasks to generate in one epoch\n",
        "            n_shot: int. Number of samples for each class in the n-shot classification tasks.\n",
        "            k_way: int. Number of classes in the n-shot classification tasks.\n",
        "            q_queries: int. Number query samples for each class in the n-shot classification tasks.\n",
        "            num_tasks: Number of n-shot tasks to group into a single batch\n",
        "            fixed_tasks: If this argument is specified this Sampler will always generate tasks from\n",
        "                the specified classes\n",
        "        \"\"\"\n",
        "        super(NShotTaskSampler, self).__init__(dataset)\n",
        "        self.episodes_per_epoch = episodes_per_epoch\n",
        "        self.dataset = dataset\n",
        "        if num_tasks < 1:\n",
        "            raise ValueError('num_tasks must be > 1.')\n",
        "\n",
        "        self.num_tasks = num_tasks\n",
        "        # TODO: Raise errors if initialise badly\n",
        "        self.k = k\n",
        "        self.n = n\n",
        "        self.q = q\n",
        "        # self.fixed_tasks = fixed_tasks\n",
        "\n",
        "        self.i_task = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.episodes_per_epochf\n",
        "\n",
        "    def __iter__(self):\n",
        "        for _ in range(self.episodes_per_epoch):\n",
        "            batch = []\n",
        "\n",
        "            for task in range(self.num_tasks):\n",
        "                # if self.fixed_tasks is None:\n",
        "                #     # Get random classes\n",
        "                #     episode_classes = np.random.choice(self.dataset.df['class_id'].unique(), size=self.k, replace=False)\n",
        "                # else:\n",
        "                    # Loop through classes in fixed_tasks\n",
        "                episode_classes = np.random.choice(self.dataset.df['class_id'].unique(), size=self.k, replace=False)\n",
        "                \n",
        "                df = self.dataset.df[self.dataset.df['class_id'].isin(episode_classes)]\n",
        "\n",
        "                support_k = {k: None for k in episode_classes}\n",
        "                for k in episode_classes:\n",
        "                    # Select support examples\n",
        "                    support = df[df['class_id'] == k].sample(self.n)\n",
        "                    support_k[k] = support\n",
        "\n",
        "                    for i, s in support.iterrows():\n",
        "                        batch.append(s['id'])\n",
        "\n",
        "                for k in episode_classes:\n",
        "                    query = df[(df['class_id'] == k) & (~df['id'].isin(support_k[k]['id']))].sample(self.q)\n",
        "                    for i, q in query.iterrows():\n",
        "                        batch.append(q['id'])\n",
        "\n",
        "            yield np.stack(batch)"
      ],
      "metadata": {
        "id": "awpXFMgtv_45"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SignatureDataset(Dataset):\n",
        "    def __init__(self, label_file, root_dir, transform = None):\n",
        "        '''Arguments:\n",
        "        label_file: path to csv file which contains 2 columns:\n",
        "        one with name of the person, other with the label\n",
        "        root_dir: path file of images\n",
        "        transform: transformations that will be applied (default: None)'''\n",
        "        self.df = pd.read_csv(label_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        '''lenght of the file'''\n",
        "        return len(self.df)\n",
        "    def __getitem__(self, index):\n",
        "        '''getting image based on index (protocol in MapDatasets)'''\n",
        "        images = []\n",
        "        labels = []\n",
        "        for ind in index: #since our index coming from \n",
        "            img_path = os.path.join(self.root_dir, str(self.df.iloc[ind, 0]))\n",
        "            image_i = io.imread(img_path)\n",
        "            label_i = torch.tensor(int(self.df.iloc[ind, 1]))\n",
        "\n",
        "            if self.transform:\n",
        "                '''transformations to be done to image'''\n",
        "                image_i = self.transform(image_i)\n",
        "            images.append(image_i)\n",
        "            labels.append(label_i)\n",
        "\n",
        "        #added squeeze to test it\n",
        "        label = torch.stack(labels)\n",
        "        return (images, label)"
      ],
      "metadata": {
        "id": "jHPX0-I4wQaI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n= 5\n",
        "path = '' #change this string to path where we will save the model\n",
        "k= 5 #number of classes.\n",
        "q = 5 #number of cl\n",
        "train_dataset = SignatureDataset('/content/gdrive/My Drive/train_labels.csv', '/content/gdrive/My Drive/full_org')\n",
        "val_dataset = SignatureDataset('/content/gdrive/My Drive/val_labels.csv','/content/gdrive/My Drive/full_org_val')\n",
        "fsh_sampler_train = NShotTaskSampler(dataset = train_dataset, episodes_per_epoch=100, n = n, k = k, q = q)\n",
        "fsh_sampler_val = NShotTaskSampler(dataset = val_dataset, episodes_per_epoch=100, n = n, k = k, q = q)\n",
        "  #few shot learning sampler\n",
        "train_dataloader = DataLoader(train_dataset, sampler=fsh_sampler_train) #validation dataloader\n",
        "val_dataloader = DataLoader(val_dataset, sampler=fsh_sampler_val) #validation dataloader\n",
        "\n",
        "# device = torch.device('cuda')\n",
        "# model = Matching(K = 5,input_size = 64, hidden_size=64, layers=5, n_shot= n, k_way=k, q=q, is_full_context_embedding=True) #model for this case\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3, momentum = 0.9) #optimizer for training, test ADAM as well\n",
        "# train(model, optimizer, train_dataloader, val_dataloader, n, k, q, device, 10)\n",
        "# torch.save(model, path)\n",
        "_, labels = next(iter(train_dataloader))\n",
        "print(labels.shape, labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yE4Es8sHwohC",
        "outputId": "c2ce815c-415e-404b-cc30-0d205bce63f4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 50]) tensor([[12, 12, 12, 12, 12,  5,  5,  5,  5,  5, 23, 23, 23, 23, 23, 24, 24, 24,\n",
            "         24, 24,  9,  9,  9,  9,  9, 12, 12, 12, 12, 12,  5,  5,  5,  5,  5, 23,\n",
            "         23, 23, 23, 23, 24, 24, 24, 24, 24,  9,  9,  9,  9,  9]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') #cuda device to put tensors to same device\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_channels, hidden_channels):\n",
        "        '''Encoder network to encode the various inputs to embedding space\n",
        "        Arguments\n",
        "        ---------\n",
        "        input_channels: number of channels of the input image\n",
        "        hidden_channels: number of channels of hidden features'''\n",
        "        super(Encoder, self).__init__()\n",
        "        self.input_channels = input_channels\n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(self.input_channels, self.hidden_channels, kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(self.hidden_channels),\n",
        "            nn.Conv2d(self.hidden_channels, self.hidden_channels, kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.BatchNorm2d(self.hidden_channels),\n",
        "            nn.Conv2d(self.hidden_channels, self.hidden_channels, kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.BatchNorm2d(self.hidden_channels),\n",
        "            nn.Conv2d(self.hidden_channels, self.hidden_channels, kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(self.hidden_channels),\n",
        "            nn.AdaptiveMaxPool2d((1,1)),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        list_x = []\n",
        "        for i in range(len(x)):\n",
        "            feature_i = self.main(x[i].unsqueeze(0).to(device).float())\n",
        "            list_x.append(feature_i)\n",
        "        output = torch.cat(tuple(list_x), dim = 0)\n",
        "        return output\n",
        "class AttLSTM(nn.Module):\n",
        "    def __init__(self, K, input_size, hidden_size):\n",
        "        '''attention LSTM with skip connections\n",
        "        Arguments\n",
        "        ---------\n",
        "        K: number of procesing steps\n",
        "        input_size: size of input\n",
        "        hidden_size: size of hidden features'''\n",
        "        super(AttLSTM, self).__init__()\n",
        "        self.processing = K #number of times to run lstm cells ( number of lstm cells basically)\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTMCell(self.input_size, self.hidden_size)\n",
        "        self.softmax = nn.Softmax(dim = 1)\n",
        "    def forward(self, f_x, g_S):\n",
        "        h = f_x\n",
        "        #g_S: shape (n*k, feature_size)\n",
        "        #f_X: shape: (n*q, feature_size)\n",
        "        c = torch.zeros(f_x.shape[0], f_x.shape[1]).to(device).float() #putting our tensor to device\n",
        "        for _ in range(self.processing):\n",
        "            product = torch.matmul(h, g_S.T)#product shape (nk x nq) -- > (nq x nk)\n",
        "            a = self.softmax(product) #attention\n",
        "            r = torch.matmul(a, g_S) #summation over sequential data\n",
        "            concat = h + r #concatination\n",
        "            h, c  = self.lstm(f_x, (concat, c)) #output of LSTM\n",
        "            h = h + f_x #skip connection\n",
        "        return h\n",
        "\n",
        "class Matching(nn.Module):# change attention LSTM to transformer.(i.e add transformer option) \n",
        "    #change bidirectional lstm to one with skip connection.\n",
        "    def __init__(self, K, input_size, hidden_size, layers, n_shot, k_way, q, is_full_context_embedding = False):\n",
        "        super(Matching, self).__init__()\n",
        "        self.layers = layers\n",
        "        self.processing = K\n",
        "        self.n_shot = n_shot\n",
        "        self.k_way  = k_way\n",
        "        self.q = q\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.is_full_context_embedding = is_full_context_embedding\n",
        "        self.mapping = Encoder(1, self.input_size)\n",
        "        self.biLSTM = nn.LSTM(self.input_size, self.hidden_size//2, self.layers, bidirectional = True) #set other arguments as well\n",
        "        self.attLSTM = AttLSTM(K, self.input_size, self.hidden_size) #set other arguments as well \n",
        "        # self.cos = nn.CosineSimilarity(dim=2)\n",
        "        self.softmax = nn.Softmax(dim = 1)\n",
        "    def cosine_similarity_matrix(self, x:torch.tensor,\n",
        "                                    y:torch.tensor,\n",
        "                                    eps:torch.tensor = torch.tensor(1e-8)):\n",
        "            '''Cosine similarity for matrices, whose columns are same\n",
        "            x: shape (q, e): query set\n",
        "            y: shape (s, e): support set\n",
        "            eps: for numerical stabilizity\n",
        "            output: (q,s): columns of this matrice correspond to \n",
        "            attention vector of each sample from query'''\n",
        "            matrix_product = torch.matmul(x, y.T) # (n*q, feature) x (feature, n*k) ==> (n*q, n*k)\n",
        "            # print(x.shape, y.shape, \"x, y shapes\")\n",
        "            l2_norm_x = torch.norm(x, p = 2, dim = 1).unsqueeze(1) # size of this (n*q, 1) (so column vector)\n",
        "            # print(l2_norm_x.shape, \"shape of norm of x\")\n",
        "            l2_norm_y = torch.norm(y, p = 2, dim = 1).unsqueeze(1) # (n*k, 1) column vector\n",
        "            # print(l2_norm_y.shape, \"shape of norm of y\")\n",
        "            denominator = torch.maximum(torch.matmul(l2_norm_x, l2_norm_y.T), eps) #denominator: (n*q, 1) x (1, n*k) = (n*q, n*k)\n",
        "            return 1 - matrix_product/denominator\n",
        "    def forward(self, x):\n",
        "\n",
        "        if self.is_full_context_embedding:\n",
        "            x  = self.mapping(x)\n",
        "            # print(x.shape, \"X shape here\")\n",
        "            support_set = x[:self.n_shot*self.k_way] #size of the x is (n*k + q+k, feature_size)\n",
        "            support_set = support_set.unsqueeze(1)\n",
        "            # print(support_set.shape, \"here support set shape\")\n",
        "            query_set = x[self.n_shot*self.k_way:]\n",
        "\n",
        "            g_S, _ = self.biLSTM(support_set)\n",
        "            g_S = g_S.squeeze(1)\n",
        "            # print(g_S.shape, \"Here we have g_S shape\")\n",
        "            # print(query_set.shape, \"Here shape of query set\")\n",
        "            f_x = self.attLSTM(query_set, g_S)\n",
        "            # print(f_x.shape, \"here f_x shape\")\n",
        "\n",
        "            return self.softmax(self.cosine_similarity_matrix(f_x, g_S))\n",
        "        else:\n",
        "            x = self.mapping(x)\n",
        "            support_set = x[:self.n_shot*self.k_way]\n",
        "            # support shape: (n*k, feature)\n",
        "            query_set = x[self.n_shot*self.k_way:]\n",
        "            # query shape\" (n*q, feature)\n",
        "            return self.softmax(self.cosine_similarity_matrix(query_set, support_set))\n"
      ],
      "metadata": {
        "id": "YeKq36Pa2JlA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model: Module,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          train_dataloader:DataLoader,\n",
        "          val_dataloader:DataLoader,\n",
        "        #   N_tr: int ,\n",
        "          n: int ,\n",
        "          k: int ,\n",
        "          q: int,\n",
        "          device:torch.cuda.device,\n",
        "          epochs: int):\n",
        "    '''1. find Dataset #number of classes\n",
        "       2.fill random sampler function from \n",
        "        3. fill examples_per_class\n",
        "        4. extract examples per_class'''\n",
        "    criterion = nn.NLLLoss()\n",
        "    model.train()\n",
        "    def ohe(y:torch.tensor, k):\n",
        "                #one hot encoding of labels\n",
        "        y = y.squeeze(0).to(device)\n",
        "        y_shape = y.shape\n",
        "        classes = torch.unique(y).to(device).long() #sorted tensor containing classes\n",
        "        # tpl = (y_shape, k)\n",
        "        for i in range(classes.shape[0]):\n",
        "          y[(y == classes[i]).nonzero(as_tuple=True)[0]] = i\n",
        "        return y\n",
        "        # one_hot_y = torch.zeros(tpl).to(device).float()\n",
        "        # values = []\n",
        "        # for i, label in enumerate(y):\n",
        "        #     if label not in values:\n",
        "        #         values.append(label)\n",
        "        #         k = len(values)\n",
        "        #         one_hot_y[i, k] = 1\n",
        "        #     else:\n",
        "        #         k = values.index(label)\n",
        "        #         one_hot_y[i, k] = 1\n",
        "        # return one_hot_y\n",
        "\n",
        "    loss_total = 0\n",
        "    accuracy = []\n",
        "    for epoch in range(epochs):\n",
        "        with tqdm(train_dataloader, unit=\"support\") as tepoch:\n",
        "            for (x, label) in tepoch:\n",
        "                '''shape of x: (batch, n*k + q*k, channels, height, width)'''\n",
        "                label.to(device).float()\n",
        "                # print(label.to(device))\n",
        "                # label.squeeze(0)\n",
        "\n",
        "                new_y = ohe(label, k).long() #new labels ranging from 0 to k\n",
        "                # index_error = new_y.clone().long() #new labels ranging from 0 to k\n",
        "                one_hot_y = F.one_hot(new_y).float() #one_hot encoding of the new labels \n",
        "                y_S = new_y[:n*k]\n",
        "                one_y_S = one_hot_y[:n*k] #getting labels of support set\n",
        "                y = new_y[n*k:]#getting labels of query set note that this one_hot encoded y\n",
        "                one_y = one_hot_y[n*k:] #one hot labels of query set\n",
        "                attention = model(x) #outputting attention\n",
        "                # print(attention, 'attention tensor')\n",
        "                y_hat = torch.matmul(attention, one_y_S)#getting predicted labels for query set\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                # print(y_hat, \"HERE Y_HAT\")\n",
        "                # print(y, \"HERE LABEL\")\n",
        "                print(y_hat.shape, y.shape, \"shape of y and y_hat\") \n",
        "                loss = criterion(y_hat, y)\n",
        "                # print(loss, \"HERE LOSS\")\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                # pred = torch.zeros_like(y_hat).to(device).float()\n",
        "                # pred = pred.fill_(1, torch.argmax(y_hat, dim = 1))\n",
        "                pred = torch.argmax(y_hat, dim = 1)\n",
        "                loss_total += loss.item()\n",
        "                acc = (pred == y).sum().item()/y.shape[0]\n",
        "                accuracy.append(acc)\n",
        "                accuracy_total = sum(accuracy)/len(accuracy)\n",
        "                tepoch.set_postfix(loss=loss.item(), accuracy_total=100. * accuracy_total)\n",
        "                # sleep(0.1)\n",
        "\n",
        "            # if batch_idx % 50 == 0:\n",
        "            #     print(f'Epoch : {epoch} || {batch_idx}/{len(C)} || \\\n",
        "            #     loss : {loss.item():.3f}, accuracy : {accuracy * 100:.3f})\n",
        "\n",
        "            \n",
        "\n",
        "        model.eval()\n",
        "        valid_loss_total = 0\n",
        "        valid_accuracy = []\n",
        "        with torch.no_grad():\n",
        "            with tqdm(train_dataloader, unit=\"support\") as tepoch:\n",
        "                for (x, label) in val_dataloader:\n",
        "\n",
        "                    label = label.to(device).float()\n",
        "\n",
        "                    new_y = ohe(label, k).long() #new labels ranging from 0 to k\n",
        "                    # index_error = new_y.clone().long() #new labels ranging from 0 to k\n",
        "                    one_hot_y = F.one_hot(new_y).float() #one_hot encoding of the new labels \n",
        "                    y_S = new_y[:n*k]\n",
        "                    one_y_S = one_hot_y[:n*k] #getting labels of support set\n",
        "                    y = new_y[n*k:]#getting labels of query set note that this one_hot encoded y\n",
        "                    one_y = one_hot_y[n*k:] #one hot labels of query set\n",
        "                    attention = model(x) #outputting attention\n",
        "                    y_hat = torch.matmul(attention.T, one_y)#getting predicted labels for query set\n",
        "\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    loss = criterion(y_hat, y)\n",
        "                    # pred = torch.zeros_like(y_hat).to(device).float()\n",
        "                    # pred = pred.fill_(1, torch.argmax(y_hat, dim = 1))\n",
        "                    pred = torch.argmax(y_hat, dim = 1)\n",
        "                    valid_loss_total += loss.item()\n",
        "                    valid_acc = (pred == y).sum().item()/y.shape[0] #write accuracy function for this, next time add F1 scores for this\n",
        "                    valid_accuracy.append(valid_acc)\n",
        "\n",
        "                    tepoch.set_postfix(loss=valid_loss_total, accuracy=100. * valid_acc)\n"
      ],
      "metadata": {
        "id": "Df20WZIzwv_8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "# del variables\n",
        "gc.collect()\n",
        "n= 2\n",
        "path = '' #change this string to path where we will save the model\n",
        "k= 2 #number of classes.\n",
        "q = 3 #number of cl\n",
        "train_dataset = SignatureDataset('/content/gdrive/My Drive/train_labels.csv', '/content/gdrive/My Drive/full_org')\n",
        "val_dataset = SignatureDataset('/content/gdrive/My Drive/val_labels.csv','/content/gdrive/My Drive/full_org_val')\n",
        "fsh_sampler_train = NShotTaskSampler(dataset = train_dataset, episodes_per_epoch=5, n = n, k = k, q = q)\n",
        "fsh_sampler_val = NShotTaskSampler(dataset = val_dataset, episodes_per_epoch=5, n = n, k = k, q = q)\n",
        "  #few shot learning sampler\n",
        "train_dataloader = DataLoader(train_dataset, sampler=fsh_sampler_train) #validation dataloader\n",
        "val_dataloader = DataLoader(val_dataset, sampler=fsh_sampler_val) #validation dataloader\n",
        "\n",
        "device = torch.device('cuda')\n",
        "model = Matching(K = 5,input_size = 64, hidden_size=64, layers=5, n_shot= n, k_way=k, q=q, is_full_context_embedding=False).to(device) #model for this case\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3, momentum = 0.9) #optimizer for training, test ADAM as well\n",
        "train(model, optimizer, train_dataloader, val_dataloader, n, k, q, device, 10)\n",
        "# torch.save(model, path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "CKdBVfYgw6nF",
        "outputId": "35fee45e-f41c-431b-c3e4-2b691e227191"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-7600d3ea10a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMatching\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_shot\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_way\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_full_context_embedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#model for this case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#optimizer for training, test ADAM as well\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m     def register_backward_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    923\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    924\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 925\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_name = 'classifier.pt'\n",
        "path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "metadata": {
        "id": "zaBs7TBPxvy_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}