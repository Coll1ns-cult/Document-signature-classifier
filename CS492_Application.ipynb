{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ul5MUmJEwZun"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The followings are checkpoints for the models\n",
        "\n",
        "Faster R-CNN: https://drive.google.com/file/d/1WAbR50Eows_lDsevBR8aUV9-RTRKFZ2O/view?usp=sharing\n",
        "\n",
        "Prototypical Network with ResNet Backbone:https://drive.google.com/file/d/1--ZwmUJAvYlCForsYBtLG-Nbi-kelldr/view?usp=sharing\n",
        "\n",
        "Matching Network with ResNet Backbone: https://drive.google.com/file/d/1-6Lxza8lPvnURCJrdG-8MGEEtSeptJeX/view?usp=sharing\n"
      ],
      "metadata": {
        "id": "DG34r98Kh2hD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVuRNROOLdNy"
      },
      "outputs": [],
      "source": [
        "!pip install easyfsl\n",
        "!pip install gradio\n",
        "!pip install git+https://github.com/facebookresearch/detectron2.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gf29o-V0caZk"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import pandas as pd \n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as T\n",
        "from skimage import io\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnet18\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Sampler\n",
        "from torch.nn import Module\n",
        "from easyfsl.utils import plot_images, sliding_average\n",
        "import torch.optim \n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from time import sleep\n",
        "\n",
        "import gradio as gr\n",
        "import glob\n",
        "import sys\n",
        "from math import tan, pi\n",
        "import torchvision\n",
        "import cv2\n",
        "\n",
        "\n",
        "\n",
        "import json\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from detectron2.structures import BoxMode\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "\n",
        "\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultTrainer, DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import ColorMode, Visualizer\n",
        "\n",
        "import shutil\n",
        "from PIL import Image\n",
        "from tkinter import Tk\n",
        "from tkinter.filedialog import askdirectory\n",
        "import csv\n",
        "from openpyxl import Workbook\n",
        "\n",
        "#TODO HOOKS\n",
        "from detectron2.engine.hooks import HookBase\n",
        "from detectron2.evaluation import inference_context\n",
        "from detectron2.utils.logger import log_every_n_seconds\n",
        "from detectron2.data import DatasetMapper, build_detection_test_loader\n",
        "import detectron2.utils.comm as comm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backbone\n",
        "This is ResNet18 Backbone used for application"
      ],
      "metadata": {
        "id": "ak9BC4-kyWnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class resnet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(resnet, self).__init__()\n",
        "        self.resnet_18 = resnet18(pretrained=False)\n",
        "        self.resnet_18.fc = nn.Flatten()\n",
        "    def forward(self, x):\n",
        "\n",
        "        if x.shape[1] == 3:\n",
        "          return self.resnet_18(x)\n",
        "        elif x.shape[1] == 1:\n",
        "          x = torch.cat((x, x, x), dim = 1)\n",
        "          return self.resnet_18(x)\n",
        "        else:\n",
        "          raise ValueError('shape[1] is not 1 or 3 or it is not even channel dimension')"
      ],
      "metadata": {
        "id": "qW51Cj1HxNw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prototypical Networks with ResNet-18 Backbone"
      ],
      "metadata": {
        "id": "7hKI9m9vygZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Proto(nn.Module):\n",
        "    def __init__(self, hidden_channels, input_channels\n",
        "                 #, n_shot, k_way, q\n",
        "                 ):\n",
        "        super(Proto, self).__init__()\n",
        "        # self.n_shot = n_shot\n",
        "        # self.k_way = k_way\n",
        "        # self.q = q\n",
        "        self.input_channels = input_channels\n",
        "        self.hidden_channels = hidden_channels\n",
        "        # self.backbone = ConvNet(self.input_channels, self.hidden_channels)\n",
        "        self.backbone = resnet()\n",
        "    def forward(self, x, n_shot, k_way, q):\n",
        "        #here y_support is already made label:\n",
        "        x = x.squeeze(0).to(device)\n",
        "        embedded_x = self.backbone(x)\n",
        "        # print(embedded_x.shape)\n",
        "        support_set = embedded_x[:k_way*n_shot] #shape of n*k, embedding dim \n",
        "        q_set = embedded_x[k_way*n_shot:] #shape of q*k, embedding dim\n",
        "        mean_support = torch.cat([torch.mean(support_set[i*n_shot:(i+1)*n_shot], dim = 0).unsqueeze(0) for i in range(k_way)]) #now we have \n",
        "        # print(q_set.shape, mean_support.shape)\n",
        "        l2_distance = torch.cdist(q_set, mean_support)\n",
        "        # print(l2_distance.shape, 'shape of l2 distance matrix')\n",
        "        return -l2_distance"
      ],
      "metadata": {
        "id": "qVvFZVMzxPE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Device\n",
        "Please set the device as you want"
      ],
      "metadata": {
        "id": "qmLGyVvSyqwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda')"
      ],
      "metadata": {
        "id": "wywRavfaxRnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weight Loading\n",
        "Load the weights for protypical networks"
      ],
      "metadata": {
        "id": "pxEBVun6y0no"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Proto(32, 1).to(device)\n",
        "model.load_state_dict(torch.load('/content/gdrive/My Drive/Proto_ResNet.pth', map_location=device))"
      ],
      "metadata": {
        "id": "sNtiVUz4xXVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detection"
      ],
      "metadata": {
        "id": "WCNgE0Rhy_fN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO NOT FREEZE\n",
        "def run():\n",
        "    torch.multiprocessing.freeze_support()\n",
        "    print('loop')\n",
        "\n",
        "# TODO GETTING DATA\n",
        "def get_data_dicts(directory, classes):\n",
        "    dataset_dicts = []\n",
        "    for filename in [file for file in os.listdir(directory) if file.endswith('.json')]:\n",
        "        json_file = os.path.join(directory, filename)\n",
        "        with open(json_file) as f:\n",
        "            img_anns = json.load(f)\n",
        "\n",
        "        record = {}\n",
        "\n",
        "        filename = os.path.join(directory, img_anns[\"imagePath\"])\n",
        "\n",
        "        record[\"file_name\"] = filename\n",
        "        record[\"height\"] = 800\n",
        "        record[\"width\"] = 800\n",
        "\n",
        "        annos = img_anns[\"shapes\"]\n",
        "        objs = []\n",
        "        for anno in annos:\n",
        "            px = [a[0] for a in anno['points']]  # x coord\n",
        "            py = [a[1] for a in anno['points']]  # y-coord\n",
        "            poly = [(x, y) for x, y in zip(px, py)]  # poly for segmentation\n",
        "            poly = [p for x in poly for p in x]\n",
        "\n",
        "            obj = {\n",
        "                \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n",
        "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
        "                \"segmentation\": [poly],\n",
        "                \"category_id\": classes.index(anno['label']),\n",
        "                \"iscrowd\": 0\n",
        "            }\n",
        "            objs.append(obj)\n",
        "        record[\"annotations\"] = objs\n",
        "        dataset_dicts.append(record)\n",
        "    return dataset_dicts\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run()\n",
        "\n",
        "\n",
        "# # TODO CLASS AND SHIT\n",
        "classes = ['signature', 'date']\n",
        "\n",
        "data_path = 'C:/Users/kamra/Documents/My Documents/2022 Fall/Deep Learning/Project/ds_reformatted1/'\n",
        "\n",
        "for d in [\"train\", \"test\"]:\n",
        "    DatasetCatalog.register(\n",
        "        \"category_\" + d,\n",
        "        lambda d=d: get_data_dicts(data_path+d, classes)\n",
        "    )\n",
        "    MetadataCatalog.get(\"category_\" + d).set(thing_classes=classes)\n",
        "\n",
        "microcontroller_metadata = MetadataCatalog.get(\"category_train\")\n",
        "\n",
        "\n",
        "# # TODO TRAIN OPTIONS\n",
        "cfg = get_cfg()\n",
        "cfg.MODEL.DEVICE = \"cpu\"\n",
        "\n",
        "# TODO TEST OPTIONS\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"/content/gdrive/My Drive/model_final.pth\")\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.6\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# TODO TEST ONLY\n",
        "\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def out(imageName):\n",
        "    im = np.array(imageName)\n",
        "    im2 = imageName\n",
        "    outputs = predictor(im)\n",
        "    v = Visualizer(im[:, :, ::-1],\n",
        "                    metadata=microcontroller_metadata)\n",
        "    #\n",
        "    pred_classes = outputs['instances'].pred_classes.cpu().tolist()\n",
        "    class_names = MetadataCatalog.get(\"category_train\").thing_classes\n",
        "    pred_class_names = list(map(lambda x: class_names[x], pred_classes))\n",
        "    #Save detected image\n",
        "    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    #v.save(imageName + \"_test.png\")\n",
        "    # Cropping\n",
        "    i = 0\n",
        "    for pred in range(len(pred_class_names)):\n",
        "        if pred_class_names[pred] == \"signature\":\n",
        "            break\n",
        "        else:\n",
        "            i += 1\n",
        "    boxes = list(outputs[\"instances\"].pred_boxes)\n",
        "    box = boxes[i]\n",
        "    box = box.detach().cpu().numpy()\n",
        "    x_top_left = box[0]\n",
        "    y_top_left = box[1]\n",
        "    x_bottom_right = box[2]\n",
        "    y_bottom_right = box[3]\n",
        "    crop_img = im2.crop((int(x_top_left), int(y_top_left), int(x_bottom_right), int(y_bottom_right)))\n",
        "    # print(int(x_top_left),int(x_bottom_right), int(y_bottom_right), int(y_top_left), \"Results for this\")\n",
        "    # print(im2.shape)\n",
        "    # crop_img = im2[int(x_top_left):int(x_bottom_right), int(y_top_left):int(y_bottom_right)]\n",
        "    #crop_img.save(imageName + \"_cropped.png\")\n",
        "    return v, crop_img"
      ],
      "metadata": {
        "id": "4Ogp8BmtxjtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detection Application\n",
        "This application is for only detection, given the document to the left window, signature detected document, and cropped part for further processing will be out"
      ],
      "metadata": {
        "id": "muTeLq6NzEvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detection(image):\n",
        "  v, cropped_image = out(image)\n",
        "  v = Image.fromarray(np.uint8(v.get_image())).convert('RGB')\n",
        "  return v, cropped_image\n",
        "\n",
        "\n",
        "demo_detection = gr.Interface(\n",
        "    fn=detection,\n",
        "    inputs=[gr.Image(type=\"pil\")],\n",
        "    outputs =[gr.Image(), gr.Image()]\n",
        ")\n",
        "demo_detection.launch(debug= True)"
      ],
      "metadata": {
        "id": "w8oPAtfXxnDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Application for both Registering and Classfying\n",
        "Please be careful that you don't register two same signatures since model is based one-shot learning.\n",
        "\n",
        "Firstly register only images containing signatures only, after loading at least two images, you can test the classification in Detection and Classification window by uploading the document"
      ],
      "metadata": {
        "id": "0fBkSbTvzZI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "signatures = [] #signatures to be stores \n",
        "names = [] #carrier of the signatures\n",
        "def label_to_name(names):\n",
        "  return {name:i for i, name in enumerate(names)}\n",
        "\n",
        "transform = T.Compose([T.ToTensor(), T.Grayscale(), T.Resize((200, 200))])\n",
        "def registering_new_signature(name,image):\n",
        "    image = transform(image)\n",
        "    signatures.append(image)\n",
        "    names.append(name)\n",
        "    return str(image.shape[0]) + \",\" + str(image.shape[1]) + \",\" + str(image.shape[2])\n",
        "\n",
        "def classify_signature(image):\n",
        "\n",
        "    v, cropped_image = out(image)\n",
        "    v = Image.fromarray(np.uint8(v.get_image())).convert('RGB')\n",
        "    image = transform(cropped_image)\n",
        "    x = torch.cat((torch.stack(signatures), image.unsqueeze(0)))\n",
        "\n",
        "    probs = torch.softmax(model(x, 1, len(signatures), 1), dim = 1).squeeze(0)\n",
        "\n",
        "\n",
        "    return {str(names[i]):float(probs[i]) for i in range(len(signatures))}, v\n",
        "    \n",
        "demo_register = gr.Interface(\n",
        "    fn=registering_new_signature,\n",
        "    inputs=['text', gr.Image()],\n",
        "    outputs = \"text\"\n",
        ")\n",
        "\n",
        "demo_classifier = gr.Interface(\n",
        "    fn = classify_signature,\n",
        "    inputs = gr.Image(type = 'pil'),\n",
        "    outputs=[gr.Label(), gr.Image()]\n",
        ")\n",
        "\n",
        "demo = gr.TabbedInterface([demo_register, demo_classifier], [\"Registering\", \"Detection & Classification\"])\n",
        "print(names)\n",
        "demo.launch(debug= True)"
      ],
      "metadata": {
        "id": "bAnPHT1Rx7n2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}