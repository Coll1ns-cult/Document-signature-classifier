{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "LJT2wPKtb7MU",
        "outputId": "ee0cffe3-875b-4d7c-a963-c5a55e277d8e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4996ee3d8d09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    122\u001b[0m       'TBE_EPHEM_CREDS_ADDR'] if ephemeral else _os.environ['TBE_CREDS_ADDR']\n\u001b[1;32m    123\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    125\u001b[0m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   request_id = send_request(\n\u001b[1;32m    170\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    100\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    101\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The followings are checkpoints for the models\n",
        "\n",
        "Faster R-CNN: https://drive.google.com/file/d/1WAbR50Eows_lDsevBR8aUV9-RTRKFZ2O/view?usp=sharing\n",
        "\n",
        "Prototypical Network with ResNet Backbone:https://drive.google.com/file/d/1--ZwmUJAvYlCForsYBtLG-Nbi-kelldr/view?usp=sharing\n",
        "\n",
        "Matching Network with ResNet Backbone: https://drive.google.com/file/d/1-6Lxza8lPvnURCJrdG-8MGEEtSeptJeX/view?usp=sharing\n"
      ],
      "metadata": {
        "id": "DG34r98Kh2hD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVuRNROOLdNy",
        "outputId": "e494b74b-377e-4818-9014-97106912a352"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: easyfsl in /usr/local/lib/python3.8/dist-packages (1.1.0)\n",
            "Requirement already satisfied: torchvision>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from easyfsl) (0.14.0+cu116)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from easyfsl) (1.13.0+cu116)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from easyfsl) (1.3.5)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from easyfsl) (3.2.2)\n",
            "Requirement already satisfied: tqdm>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from easyfsl) (4.64.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.0.0->easyfsl) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.0.0->easyfsl) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.0.0->easyfsl) (1.21.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.0.0->easyfsl) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.0.0->easyfsl) (1.4.4)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.1.0->easyfsl) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0.0->easyfsl) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.4.0->easyfsl) (4.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.7.0->easyfsl) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.7.0->easyfsl) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.7.0->easyfsl) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.7.0->easyfsl) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.7.0->easyfsl) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.7.0->easyfsl) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.8/dist-packages (3.12.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.8/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from gradio) (3.8.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from gradio) (3.2.2)\n",
            "Requirement already satisfied: markdown-it-py[linkify,plugins] in /usr/local/lib/python3.8/dist-packages (from gradio) (2.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from gradio) (6.0)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.8/dist-packages (from gradio) (0.0.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from gradio) (1.3.5)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.8/dist-packages (from gradio) (3.8.3)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.8/dist-packages (from gradio) (3.16.0)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.8/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from gradio) (2022.11.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.8/dist-packages (from gradio) (0.88.0)\n",
            "Requirement already satisfied: paramiko in /usr/local/lib/python3.8/dist-packages (from gradio) (2.12.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from gradio) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from gradio) (1.21.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from gradio) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from gradio) (2.23.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.8/dist-packages (from gradio) (0.3.0)\n",
            "Requirement already satisfied: websockets>=10.0 in /usr/local/lib/python3.8/dist-packages (from gradio) (10.4)\n",
            "Requirement already satisfied: h11<0.13,>=0.11 in /usr/local/lib/python3.8/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.8/dist-packages (from gradio) (0.23.1)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.8/dist-packages (from gradio) (1.10.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (1.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (2.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (6.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (22.1.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.8/dist-packages (from yarl<2.0,>=1.0->aiohttp->gradio) (2.10)\n",
            "Requirement already satisfied: starlette==0.22.0 in /usr/local/lib/python3.8/dist-packages (from fastapi->gradio) (0.22.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from starlette==0.22.0->fastapi->gradio) (3.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.8/dist-packages (from starlette==0.22.0->fastapi->gradio) (4.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.8/dist-packages (from anyio<5,>=3.4.0->starlette==0.22.0->fastapi->gradio) (1.3.0)\n",
            "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in /usr/local/lib/python3.8/dist-packages (from httpx->gradio) (0.15.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from httpx->gradio) (2022.9.24)\n",
            "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /usr/local/lib/python3.8/dist-packages (from httpx->gradio) (1.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->gradio) (2.0.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.8/dist-packages (from markdown-it-py[linkify,plugins]->gradio) (0.1.2)\n",
            "Requirement already satisfied: linkify-it-py~=1.0 in /usr/local/lib/python3.8/dist-packages (from markdown-it-py[linkify,plugins]->gradio) (1.0.3)\n",
            "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.8/dist-packages (from markdown-it-py[linkify,plugins]->gradio) (0.3.3)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.8/dist-packages (from linkify-it-py~=1.0->markdown-it-py[linkify,plugins]->gradio) (1.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->gradio) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->gradio) (2022.6)\n",
            "Requirement already satisfied: pynacl>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from paramiko->gradio) (1.5.0)\n",
            "Requirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.8/dist-packages (from paramiko->gradio) (38.0.4)\n",
            "Requirement already satisfied: bcrypt>=3.1.3 in /usr/local/lib/python3.8/dist-packages (from paramiko->gradio) (4.0.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.8/dist-packages (from cryptography>=2.5->paramiko->gradio) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.12->cryptography>=2.5->paramiko->gradio) (2.21)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->gradio) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->gradio) (1.24.3)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.8/dist-packages (from uvicorn->gradio) (8.1.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-cs84y9wi\n",
            "  Running command git clone -q https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-cs84y9wi\n"
          ]
        }
      ],
      "source": [
        "!pip install easyfsl\n",
        "!pip install gradio\n",
        "!pip install git+https://github.com/facebookresearch/detectron2.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gf29o-V0caZk"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import pandas as pd \n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as T\n",
        "from skimage import io\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnet18\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Sampler\n",
        "from torch.nn import Module\n",
        "from easyfsl.utils import plot_images, sliding_average\n",
        "import torch.optim \n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from time import sleep\n",
        "\n",
        "\n",
        "import glob\n",
        "import sys\n",
        "from math import tan, pi\n",
        "import torchvision\n",
        "import cv2\n",
        "\n",
        "\n",
        "\n",
        "import json\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from detectron2.structures import BoxMode\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "\n",
        "\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultTrainer, DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import ColorMode, Visualizer\n",
        "\n",
        "import shutil\n",
        "from PIL import Image\n",
        "from tkinter import Tk\n",
        "from tkinter.filedialog import askdirectory\n",
        "import csv\n",
        "from openpyxl import Workbook\n",
        "\n",
        "#TODO HOOKS\n",
        "from detectron2.engine.hooks import HookBase\n",
        "from detectron2.evaluation import inference_context\n",
        "from detectron2.utils.logger import log_every_n_seconds\n",
        "from detectron2.data import DatasetMapper, build_detection_test_loader\n",
        "import detectron2.utils.comm as comm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thT1MRsfv3fS"
      },
      "outputs": [],
      "source": [
        "class SignatureDataset(Dataset):\n",
        "    def __init__(self, label_file, root_dir, transform = None):\n",
        "        '''Arguments:\n",
        "        label_file: path to csv file which contains 2 columns:\n",
        "        one with name of the person, other with the label\n",
        "        root_dir: path file of images\n",
        "        transform: transformations that will be applied (default: None)'''\n",
        "        self.df = pd.read_csv(label_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        '''lenght of the file'''\n",
        "        return len(self.df)\n",
        "    def __getitem__(self, index):\n",
        "        '''getting image based on index (protocol in MapDatasets)'''\n",
        "        images = []\n",
        "        labels = []\n",
        "\n",
        "        for ind in index: #since our index coming from \n",
        "            img_path = os.path.join(self.root_dir, str(self.df.iloc[ind, 0]))\n",
        "            image_i = io.imread(img_path)\n",
        "            label_i = torch.tensor(int(self.df.iloc[ind, 1]))\n",
        "\n",
        "            if self.transform:\n",
        "                '''transformations to be done to image'''\n",
        "                image_i = self.transform(image_i)\n",
        "            images.append(image_i)\n",
        "            labels.append(label_i)\n",
        "\n",
        "        #added squeeze to test it\n",
        "        label = torch.stack(labels)\n",
        "        images = torch.stack(images)\n",
        "        return (images, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzaYNilTvzoH"
      },
      "outputs": [],
      "source": [
        "class NShotTaskSampler(Sampler):\n",
        "    def __init__(self,\n",
        "                 dataset: torch.utils.data.Dataset,\n",
        "                 episodes_per_epoch: int = None,\n",
        "                 n: int = None,\n",
        "                 k: int = None,\n",
        "                 q: int = None,\n",
        "                 num_tasks: int = 1\n",
        "                #  fixed_tasks: List[Iterable[int]] = None\n",
        "    ):\n",
        "        \"\"\"PyTorch Sampler subclass that generates batches of n-shot, k-way, q-query tasks.\n",
        "\n",
        "        Each n-shot task contains a \"support set\" of `k` sets of `n` samples and a \"query set\" of `k` sets\n",
        "        of `q` samples. The support set and the query set are all grouped into one Tensor such that the first n * k\n",
        "        samples are from the support set while the remaining q * k samples are from the query set.\n",
        "\n",
        "        The support and query sets are sampled such that they are disjoint i.e. do not contain overlapping samples.\n",
        "\n",
        "        # Arguments\n",
        "            dataset: Instance of torch.utils.data.Dataset from which to draw samples\n",
        "            episodes_per_epoch: Arbitrary number of batches of n-shot tasks to generate in one epoch\n",
        "            n_shot: int. Number of samples for each class in the n-shot classification tasks.\n",
        "            k_way: int. Number of classes in the n-shot classification tasks.\n",
        "            q_queries: int. Number query samples for each class in the n-shot classification tasks.\n",
        "            num_tasks: Number of n-shot tasks to group into a single batch\n",
        "            fixed_tasks: If this argument is specified this Sampler will always generate tasks from\n",
        "                the specified classes\n",
        "        \"\"\"\n",
        "        super(NShotTaskSampler, self).__init__(dataset)\n",
        "        self.episodes_per_epoch = episodes_per_epoch\n",
        "        self.dataset = dataset\n",
        "        if num_tasks < 1:\n",
        "            raise ValueError('num_tasks must be > 1.')\n",
        "\n",
        "        self.num_tasks = num_tasks\n",
        "        # TODO: Raise errors if initialise badly\n",
        "        self.k = k\n",
        "        self.n = n\n",
        "        self.q = q\n",
        "        # self.fixed_tasks = fixed_tasks\n",
        "\n",
        "        self.i_task = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.episodes_per_epoch\n",
        "\n",
        "    def __iter__(self):\n",
        "        for _ in range(self.episodes_per_epoch):\n",
        "            batch = []\n",
        "\n",
        "            for task in range(self.num_tasks):\n",
        "                # if self.fixed_tasks is None:\n",
        "                #     # Get random classes\n",
        "                #     episode_classes = np.random.choice(self.dataset.df['class_id'].unique(), size=self.k, replace=False)\n",
        "                # else:\n",
        "                    # Loop through classes in fixed_tasks\n",
        "                # print(self.dataset.df['class_id'].unique())\n",
        "                episode_classes = np.random.choice(self.dataset.df['class_id'].unique(), size=self.k, replace=False)\n",
        "                # print(episode_classes, \"EPISODE CLASSES HERE \")\n",
        "                df = self.dataset.df[self.dataset.df['class_id'].isin(episode_classes)]\n",
        "\n",
        "                support_k = {k: None for k in episode_classes}\n",
        "                for k in episode_classes:\n",
        "                    # Select support examples\n",
        "                    support = df[df['class_id'] == k].sample(self.n)\n",
        "                    # print(support, \"HERE SUPPORT SET\")\n",
        "                    support_k[k] = support\n",
        "\n",
        "                    for i, s in support.iterrows():\n",
        "                        # print(s['id'], \"INSIDE OF SAMPLER\")\n",
        "                        batch.append(s['id'])\n",
        "\n",
        "                # for k in episode_classes:\n",
        "                #     query = df[(df['class_id'] == k) & (~df['id'].isin(support_k[k]['id']))].sample(self.q)\n",
        "                #     for i, q in query.iterrows():\n",
        "                #         # print(q['id'], \"INSIDE OF SAMPLER\")\n",
        "                #         batch.append(q['id'])\n",
        "                query = df[~df['id'].isin(batch)].sample(self.q)\n",
        "                for i, q in query.iterrows():\n",
        "                    batch.append(q['id'])\n",
        "\n",
        "            yield np.stack(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktBjrsP-c9FT"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsBm9Idqc-QQ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yA9Yc5tjcR6l"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, input_channels, hidden_channels):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.input_channels = input_channels\n",
        "        self.hidden_channels = hidden_channels\n",
        "        # self.conv1 = nn.Sequential(\n",
        "        #     nn.Conv2d(self.input_channels, self.hidden_channels, kernel_size = 3),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.MaxPool2d(kernel_size = 3, stride = 2)\n",
        "        # )\n",
        "        # self.conv2 = nn.Sequential(\n",
        "        #     nn.Conv2d(self.hidden_channels, self.hidden_channels, kernel_size = 3),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.MaxPool2d(kernel_size = 3, stride = 2)\n",
        "        # )\n",
        "        # self.conv3 = nn.Sequential(\n",
        "        #     nn.Conv2d(self.hidden_channels, self.hidden_channels, kernel_size = 3),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.MaxPool2d(kernel_size = 3, stride = 2)\n",
        "        # )\n",
        "        # self.conv4 = nn.Sequential(\n",
        "        #     nn.Conv2d(self.hidden_channels, self.hidden_channels, kernel_size = 3),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.MaxPool2d(kernel_size = 3, stride = 2)\n",
        "        # )\n",
        "        # self.conv5 = nn.Sequential(\n",
        "        #     nn.Conv2d(self.hidden_channels, self.hidden_channels, kernel_size = 3),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.MaxPool2d(kernel_size = 3, stride = 2)\n",
        "        # )\n",
        "        # self.layers = nn.ModuleList([self.conv1, self.conv2, self.conv3,self.conv4,self.conv5, nn.Flatten()])\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(self.input_channels, self.hidden_channels, kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(self.hidden_channels),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2),\n",
        "            nn.Conv2d(self.hidden_channels, self.hidden_channels, kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(self.hidden_channels),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2),\n",
        "            nn.Conv2d(self.hidden_channels, self.hidden_channels, kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(self.hidden_channels),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2),\n",
        "            nn.Conv2d(self.hidden_channels, self.hidden_channels, kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(self.hidden_channels),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2),\n",
        "            nn.Conv2d(self.hidden_channels, self.hidden_channels, kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(self.hidden_channels),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2),\n",
        "            # nn.AdaptiveMaxPool2d((1,1)),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # list_x = []\n",
        "        # for i in range(len(x)):\n",
        "        #     feature_i = self.main(x[i].unsqueeze(0).to(device).float())\n",
        "        #     list_x.append(feature_i)\n",
        "        # output = torch.cat(tuple(list_x), dim = 0)\n",
        "        # return output\n",
        "        # for layer in self.layers:\n",
        "        #     x = layer(x)\n",
        "        # return x\n",
        "        return self.main(x)\n",
        "\n",
        "      # return output\n",
        "class resnet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(resnet, self).__init__()\n",
        "        self.resnet_18 = resnet18(pretrained=False)\n",
        "        self.resnet_18.fc = nn.Flatten()\n",
        "    def forward(self, x):\n",
        "\n",
        "        if x.shape[1] == 3:\n",
        "          return self.resnet_18(x)\n",
        "        elif x.shape[1] == 1:\n",
        "          x = torch.cat((x, x, x), dim = 1)\n",
        "          return self.resnet_18(x)\n",
        "        else:\n",
        "          raise ValueError('shape[1] is not 1 or 3 or it is not even channel dimension')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_YKhjr9dkR1"
      },
      "outputs": [],
      "source": [
        "class Proto(nn.Module):\n",
        "    def __init__(self, hidden_channels, input_channels\n",
        "                 #, n_shot, k_way, q\n",
        "                 ):\n",
        "        super(Proto, self).__init__()\n",
        "        # self.n_shot = n_shot\n",
        "        # self.k_way = k_way\n",
        "        # self.q = q\n",
        "        self.input_channels = input_channels\n",
        "        self.hidden_channels = hidden_channels\n",
        "        # self.backbone = ConvNet(self.input_channels, self.hidden_channels)\n",
        "        self.backbone = resnet()\n",
        "    def forward(self, x, n_shot, k_way, q):\n",
        "        #here y_support is already made label:\n",
        "        x = x.squeeze(0).to(device)\n",
        "        embedded_x = self.backbone(x)\n",
        "        # print(embedded_x.shape)\n",
        "        support_set = embedded_x[:k_way*n_shot] #shape of n*k, embedding dim \n",
        "        q_set = embedded_x[k_way*n_shot:] #shape of q*k, embedding dim\n",
        "        mean_support = torch.cat([torch.mean(support_set[i*n_shot:(i+1)*n_shot], dim = 0).unsqueeze(0) for i in range(k_way)]) #now we have \n",
        "        # print(q_set.shape, mean_support.shape)\n",
        "        l2_distance = torch.cdist(q_set, mean_support)\n",
        "        # print(l2_distance.shape, 'shape of l2 distance matrix')\n",
        "        return -l2_distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SblRQ9llukRd"
      },
      "outputs": [],
      "source": [
        "n_shot = 1\n",
        "k_way = 5\n",
        "q = 10\n",
        "\n",
        "train_set = SignatureDataset('/content/gdrive/My Drive/train_labels.csv', '/content/gdrive/My Drive/full_org', T.Compose([T.ToTensor(), T.Resize((500, 500))]))\n",
        "val_set = SignatureDataset('/content/gdrive/My Drive/val.csv','/content/gdrive/My Drive/full_org_val_1', T.Compose([T.ToTensor(), T.Resize((500, 500))]))\n",
        "test_set = SignatureDataset('/content/gdrive/My Drive/val_labels.csv','/content/gdrive/My Drive/full_org_test', T.Compose([T.ToTensor(), T.Resize((500, 500))]))\n",
        "\n",
        "\n",
        "train_sampler = NShotTaskSampler(dataset = train_set, episodes_per_epoch = 100, n = n_shot, k = k_way, q=q)\n",
        "val_sampler = NShotTaskSampler(dataset = val_set, episodes_per_epoch = 1000, n = n_shot, k = k_way, q=q)\n",
        "test_sampler = NShotTaskSampler(dataset = test_set, episodes_per_epoch = 100, n = n_shot, k = k_way, q=q)\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(train_set, sampler = train_sampler) #validation dataloader\n",
        "val_dataloader = DataLoader(val_set, sampler=val_sampler)\n",
        "test_dataloader = DataLoader(test_set, sampler = test_sampler)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bU-92eK8Zn0D"
      },
      "outputs": [],
      "source": [
        "def not_sorted_label(y:torch.tensor):\n",
        "  if len(y.shape) == 2:\n",
        "    y = y.squeeze(0).to(device)\n",
        "  y_shape = y.shape[0]\n",
        "  # y_clone = y.detach().clone().to(device)\n",
        "  dc = {}\n",
        "  label = 0\n",
        "  for i in range(y_shape):\n",
        "    # print(y[i], \"First time\")\n",
        "    if y[i].item() in dc.keys():\n",
        "      y[i] = dc[y[i].item()]\n",
        "    else:\n",
        "      # print({y[i]:label})\n",
        "      dc.update({y[i].item(): label})\n",
        "      y[i] = label\n",
        "      label += 1\n",
        "    # print(dc)\n",
        "    # print(y[i], \"Second time\")\n",
        "  return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGwV2bPGeZdN"
      },
      "outputs": [],
      "source": [
        "model = Proto(32, 1\n",
        "              # , n_shot, k_way, q\n",
        "              ).to(device)\n",
        "# torch.cuda.empty_cache()\n",
        "import gc\n",
        "# del variables\n",
        "gc.collect()\n",
        "\n",
        "def evaluate_one_episode(x, y, n_shot, q, model):\n",
        "    # print(y.shape, \"here shape of y\")\n",
        "    k_way = (y.shape[0] - q)//n_shot\n",
        "    query_labels = y[n_shot*k_way:]\n",
        "    # print(model(x))\n",
        "    # print(query_labels.shape, \"shape of query labels\")\n",
        "    # print(torch.max(torch.softmax(model(x), dim = 1).detach().data, 1)[0])\n",
        "    return (torch.max(\n",
        "            model(x\n",
        "                  , n_shot, k_way, q\n",
        "                  )\n",
        "            .detach()\n",
        "            .data,\n",
        "            1,\n",
        "        )[1]\n",
        "        == query_labels.to(device)\n",
        "    ).sum().item(), len(query_labels)\n",
        "\n",
        "def evaluate(data_loader, model):\n",
        "    total_predictions = 0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    # eval mode affects the behaviour of some layers (such as batch normalization or dropout)\n",
        "    # no_grad() tells torch not to keep in memory the whole computational graph (it's more lightweight this way)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for episode_index, (x, y) in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
        "            y = y.squeeze(0).to(device)\n",
        "            # y_sample = y.clone().to(device)\n",
        "            # print(new_label(y_sample), \"this one\")\n",
        "            # for i in range(k_way):\n",
        "            #   y[i*n_shot:(i+1)*n_shot] = i\n",
        "            #   y[n_shot*k_way+i*q:n_shot*k_way+(i+1)*q]=i\n",
        "            # print(y)\n",
        "            y = not_sorted_label(y)\n",
        "            correct, total = evaluate_one_episode(\n",
        "                x, y, n_shot, q, model\n",
        "            )\n",
        "\n",
        "            total_predictions += total\n",
        "            correct_predictions += correct\n",
        "\n",
        "    print(\n",
        "        f\"Model tested on {len(data_loader)} tasks. Accuracy: {(100 * correct_predictions/total_predictions):.2f}%\"\n",
        "    )\n",
        "# evaluate(val_dataloader, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VM8YGDA9OHyx",
        "outputId": "449e5b99-f7ad-4455-d4d9-e607ed897fc4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.5096, 0.4904])"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# a = torch.tensor([-0.0663, -0.1048])\n",
        "# torch.softmax(a, dim = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjdSg_Q0VZCN"
      },
      "outputs": [],
      "source": [
        "# model = Proto(32, 1\n",
        "#               # , n_shot, k_way, q\n",
        "#               ).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "def fit(x, y, n_shot, q\n",
        ") -> float:\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    k_way = (y.shape[0] - q)//n_shot\n",
        "    y = y[n_shot*k_way:]\n",
        "    scores = model(x, n_shot, k_way, q)\n",
        "    loss = criterion(scores, y.to(device))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZyTvafZcV_e",
        "outputId": "f7f8b050-880a-4356-ce0b-e6ff5f0b6774"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:37<00:00,  2.68it/s, loss=0.00294]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.70it/s, loss=0.00369]\n",
            "100%|██████████| 100/100 [00:36<00:00,  2.72it/s, loss=0.00636]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.69it/s, loss=0.0131]\n",
            "100%|██████████| 100/100 [00:39<00:00,  2.55it/s, loss=0.00115]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.70it/s, loss=0.00181]\n",
            "100%|██████████| 100/100 [00:36<00:00,  2.71it/s, loss=0.00272]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.70it/s, loss=0.00287]\n",
            "100%|██████████| 100/100 [00:36<00:00,  2.75it/s, loss=0.00185]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.67it/s, loss=0.0102]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.70it/s, loss=0.0074]\n",
            "100%|██████████| 100/100 [00:36<00:00,  2.73it/s, loss=0.0316]\n",
            "100%|██████████| 100/100 [00:36<00:00,  2.73it/s, loss=0.00523]\n",
            "100%|██████████| 100/100 [00:36<00:00,  2.73it/s, loss=0.00704]\n",
            "100%|██████████| 100/100 [00:36<00:00,  2.73it/s, loss=0.00319]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.66it/s, loss=0.0281]\n",
            "100%|██████████| 100/100 [00:36<00:00,  2.75it/s, loss=0.00709]\n",
            "100%|██████████| 100/100 [00:36<00:00,  2.74it/s, loss=0.0155]\n",
            "100%|██████████| 100/100 [00:36<00:00,  2.73it/s, loss=0.0015]\n",
            "100%|██████████| 100/100 [00:36<00:00,  2.75it/s, loss=0.000697]\n",
            "100%|██████████| 100/100 [00:36<00:00,  2.74it/s, loss=0.000673]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.70it/s, loss=0.00364]\n",
            "100%|██████████| 100/100 [00:36<00:00,  2.75it/s, loss=0.000591]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.70it/s, loss=0.000801]\n",
            "100%|██████████| 100/100 [00:36<00:00,  2.75it/s, loss=0.000301]\n"
          ]
        }
      ],
      "source": [
        "CUDA_LAUNCH_BLOCKING=1\n",
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "# del variables\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "log_update_frequency = 10\n",
        "epochs = 25\n",
        "\n",
        "all_loss = []\n",
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "  with tqdm(enumerate(train_dataloader), total=len(train_dataloader)) as tqdm_train:\n",
        "      for episode_index, (x, y) in tqdm_train:\n",
        "          y = y.squeeze(0)\n",
        "          y = not_sorted_label(y)\n",
        "          # y = y[n_shot*k_way:]\n",
        "          # print(y)\n",
        "\n",
        "          loss_value = fit(x, y, n_shot, q)\n",
        "          all_loss.append(loss_value)\n",
        "\n",
        "          if episode_index % log_update_frequency == 0:\n",
        "              tqdm_train.set_postfix(loss=sliding_average(all_loss, log_update_frequency))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yn1yxUrj3sq",
        "outputId": "6b8be4d1-488a-40f1-ab28-80c7819aa2ef"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:23<00:00,  4.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model tested on 100 tasks. Accuracy: 98.20%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "evaluate(val_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTWvqTPWTbIb"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), '/content/gdrive/My Drive/Proto_ResNet.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6aFc-IlYYxb",
        "outputId": "b9c6b7e1-650f-4aef-d9cb-6069fbf709dc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [1:37:22<00:00,  5.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model tested on 1000 tasks. Accuracy: 97.55%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model2 = Proto(32, 1\n",
        "              # , n_shot, k_way, q\n",
        "              ).to(device)\n",
        "model2.load_state_dict(torch.load('/content/gdrive/My Drive/Proto_ResNet.pth', map_location=device))\n",
        "evaluate(val_dataloader, model2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "705gTe4pauma",
        "outputId": "9ed83692-34c5-415f-ebb0-f5f813f81209"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loop\n"
          ]
        }
      ],
      "source": [
        "# TODO NOT FREEZE\n",
        "def run():\n",
        "    torch.multiprocessing.freeze_support()\n",
        "    print('loop')\n",
        "\n",
        "# TODO GETTING DATA\n",
        "def get_data_dicts(directory, classes):\n",
        "    dataset_dicts = []\n",
        "    for filename in [file for file in os.listdir(directory) if file.endswith('.json')]:\n",
        "        json_file = os.path.join(directory, filename)\n",
        "        with open(json_file) as f:\n",
        "            img_anns = json.load(f)\n",
        "\n",
        "        record = {}\n",
        "\n",
        "        filename = os.path.join(directory, img_anns[\"imagePath\"])\n",
        "\n",
        "        record[\"file_name\"] = filename\n",
        "        record[\"height\"] = 800\n",
        "        record[\"width\"] = 800\n",
        "\n",
        "        annos = img_anns[\"shapes\"]\n",
        "        objs = []\n",
        "        for anno in annos:\n",
        "            px = [a[0] for a in anno['points']]  # x coord\n",
        "            py = [a[1] for a in anno['points']]  # y-coord\n",
        "            poly = [(x, y) for x, y in zip(px, py)]  # poly for segmentation\n",
        "            poly = [p for x in poly for p in x]\n",
        "\n",
        "            obj = {\n",
        "                \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n",
        "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
        "                \"segmentation\": [poly],\n",
        "                \"category_id\": classes.index(anno['label']),\n",
        "                \"iscrowd\": 0\n",
        "            }\n",
        "            objs.append(obj)\n",
        "        record[\"annotations\"] = objs\n",
        "        dataset_dicts.append(record)\n",
        "    return dataset_dicts\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run()\n",
        "\n",
        "\n",
        "# # TODO CLASS AND SHIT\n",
        "# classes = ['signature', 'date']\n",
        "\n",
        "# data_path = 'C:/Users/kamra/Documents/My Documents/2022 Fall/Deep Learning/Project/ds_reformatted1/'\n",
        "\n",
        "# for d in [\"train\", \"test\"]:\n",
        "#     DatasetCatalog.register(\n",
        "#         \"category_\" + d,\n",
        "#         lambda d=d: get_data_dicts(data_path+d, classes)\n",
        "#     )\n",
        "#     MetadataCatalog.get(\"category_\" + d).set(thing_classes=classes)\n",
        "\n",
        "# microcontroller_metadata = MetadataCatalog.get(\"category_train\")\n",
        "\n",
        "\n",
        "# TODO TRAIN OPTIONS\n",
        "cfg = get_cfg()\n",
        "cfg.MODEL.DEVICE = \"cpu\"\n",
        "#cfg.MODEL.BACKBONE.NAME = \"build_resnet_backbone\"\n",
        "#cfg.MODEL.RESNETS.DEPTH = 34\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"category_train\",)\n",
        "cfg.DATASETS.TEST = ()\n",
        "cfg.TEST.EVAL_PERIOD = 15\n",
        "cfg.DATALOADER.NUM_WORKERS = 0\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\")\n",
        "cfg.SOLVER.IMS_PER_BATCH = 8\n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "cfg.SOLVER.MAX_ITER = 250\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 2\n",
        "\n",
        "\n",
        "# TODO TEST OPTIONS\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"/content/gdrive/My Drive/model_final.pth\")\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.6\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# TODO TEST ONLY\n",
        "\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def out(imageName):\n",
        "    im = np.array(imageName)\n",
        "    im2 = imageName\n",
        "    outputs = predictor(im)\n",
        "    v = Visualizer(im[:, :, ::-1],\n",
        "                    metadata=microcontroller_metadata)\n",
        "    #\n",
        "    pred_classes = outputs['instances'].pred_classes.cpu().tolist()\n",
        "    class_names = MetadataCatalog.get(\"category_train\").thing_classes\n",
        "    pred_class_names = list(map(lambda x: class_names[x], pred_classes))\n",
        "    #Save detected image\n",
        "    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    #v.save(imageName + \"_test.png\")\n",
        "    # Cropping\n",
        "    i = 0\n",
        "    for pred in range(len(pred_class_names)):\n",
        "        if pred_class_names[pred] == \"signature\":\n",
        "            break\n",
        "        else:\n",
        "            i += 1\n",
        "    boxes = list(outputs[\"instances\"].pred_boxes)\n",
        "    box = boxes[i]\n",
        "    box = box.detach().cpu().numpy()\n",
        "    x_top_left = box[0]\n",
        "    y_top_left = box[1]\n",
        "    x_bottom_right = box[2]\n",
        "    y_bottom_right = box[3]\n",
        "    crop_img = im2.crop((int(x_top_left), int(y_top_left), int(x_bottom_right), int(y_bottom_right)))\n",
        "    # print(int(x_top_left),int(x_bottom_right), int(y_bottom_right), int(y_top_left), \"Results for this\")\n",
        "    # print(im2.shape)\n",
        "    # crop_img = im2[int(x_top_left):int(x_bottom_right), int(y_top_left):int(y_bottom_right)]\n",
        "    #crop_img.save(imageName + \"_cropped.png\")\n",
        "    return v, crop_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ITJKxVl2IpH"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def detection(image):\n",
        "  v, cropped_image = out(image)\n",
        "  v = Image.fromarray(np.uint8(v.get_image())).convert('RGB')\n",
        "  return v, cropped_image\n",
        "\n",
        "\n",
        "demo_detection = gr.Interface(\n",
        "    fn=detection,\n",
        "    inputs=[gr.Image(type=\"pil\")],\n",
        "    outputs =[gr.Image(), gr.Image()]\n",
        ")\n",
        "demo_detection.launch(debug= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzyNtY_TaJ2i"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import requests\n",
        "\n",
        "signatures = [] #signatures to be stores \n",
        "names = [] #carrier of the signatures\n",
        "def label_to_name(names):\n",
        "  return {name:i for i, name in enumerate(names)}\n",
        "\n",
        "transform = T.Compose([T.ToTensor(), T.Grayscale(), T.Resize((200, 200))])\n",
        "def registering_new_signature(name,image):\n",
        "    image = transform(image)\n",
        "    signatures.append(image)\n",
        "    names.append(name)\n",
        "    return str(image.shape[0]) + \",\" + str(image.shape[1]) + \",\" + str(image.shape[2])\n",
        "\n",
        "def classify_signature(image):\n",
        "\n",
        "    v, cropped_image = out(image)\n",
        "    v = Image.fromarray(np.uint8(v.get_image())).convert('RGB')\n",
        "    image = transform(cropped_image)\n",
        "    x = torch.cat((torch.stack(signatures), image.unsqueeze(0)))\n",
        "\n",
        "    probs = torch.softmax(model(x, 1, len(signatures), 1), dim = 1).squeeze(0)\n",
        "    print(x.shape)\n",
        "    print(probs.shape, \"here maybe error\")\n",
        "\n",
        "    return {str(names[i]):float(probs[i]) for i in range(len(signatures))}, v\n",
        "    \n",
        "demo_register = gr.Interface(\n",
        "    fn=registering_new_signature,\n",
        "    inputs=['text', gr.Image()],\n",
        "    outputs = \"text\"\n",
        ")\n",
        "\n",
        "demo_classifier = gr.Interface(\n",
        "    fn = classify_signature,\n",
        "    inputs = gr.Image(type = 'pil'),\n",
        "    outputs=[gr.Label(), gr.Image()]\n",
        ")\n",
        "\n",
        "demo = gr.TabbedInterface([demo_register, demo_classifier], [\"Registering\", \"Detection & Classification\"])\n",
        "print(names)\n",
        "demo.launch(debug= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMmAGU1Df6iB",
        "outputId": "cde216df-c85d-4a39-c22b-2640507e6b04"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [01:04<00:00,  1.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model tested on 100 tasks. Accuracy: 86.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "evaluate(val_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsb5q3nfQrE7"
      },
      "outputs": [],
      "source": [
        "class AttLSTM(nn.Module):\n",
        "    def __init__(self, K, input_size):\n",
        "        '''attention LSTM with skip connections\n",
        "        Arguments\n",
        "        ---------\n",
        "        K: number of procesing steps\n",
        "        input_size: size of input\n",
        "        hidden_size: size of hidden features'''\n",
        "        super(AttLSTM, self).__init__()\n",
        "        self.processing = K #number of times to run lstm cells ( number of lstm cells basically)\n",
        "        self.input_size = input_size\n",
        "        # self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTMCell(self.input_size, self.input_size)\n",
        "        self.softmax = nn.Softmax(dim = 1)\n",
        "    def forward(self, f_x, g_S):\n",
        "        h = f_x\n",
        "        #g_S: shape (n*k, feature_size)\n",
        "        #f_X: shape: (n*q, feature_size)\n",
        "        c = torch.zeros(f_x.shape[0], f_x.shape[1]).to(device).float() #putting our tensor to device\n",
        "        for _ in range(self.processing):\n",
        "            product = torch.matmul(h, g_S.T)#product shape (nk x nq) -- > (nq x nk)\n",
        "            a = self.softmax(product) #attention\n",
        "            r = torch.matmul(a, g_S) #summation over sequential data\n",
        "            concat = h + r #concatination\n",
        "            h, c  = self.lstm(f_x, (concat, c)) #output of LSTM\n",
        "            h = h + f_x #skip connection\n",
        "        return h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrV7NF8cQrhL"
      },
      "outputs": [],
      "source": [
        "class BidirectionalLSTM(nn.Module):\n",
        "    def __init__(self, size: int, layers: int):\n",
        "        \"\"\"Bidirectional LSTM used to generate fully conditional embeddings (FCE) of the support set as described\n",
        "        in the Matching Networks paper.\n",
        "\n",
        "        # Arguments\n",
        "            size: Size of input and hidden layers. These are constrained to be the same in order to implement the skip\n",
        "                connection described in Appendix A.2\n",
        "            layers: Number of LSTM layers\n",
        "        \"\"\"\n",
        "        super(BidirectionalLSTM, self).__init__()\n",
        "        self.num_layers = layers\n",
        "        self.batch_size = 1\n",
        "        # Force input size and hidden size to be the same in order to implement\n",
        "        # the skip connection as described in Appendix A.1 and A.2 of Matching Networks\n",
        "        self.lstm = nn.LSTM(input_size=size,\n",
        "                            num_layers=layers,\n",
        "                            hidden_size=size,\n",
        "                            bidirectional=True)\n",
        "    def forward(self, inputs):\n",
        "        # Give None as initial state and Pytorch LSTM creates initial hidden states\n",
        "        output, (hn, cn) = self.lstm(inputs, None)\n",
        "\n",
        "        forward_output = output[:, :, :self.lstm.hidden_size]\n",
        "        backward_output = output[:, :, self.lstm.hidden_size:]\n",
        "\n",
        "        # g(x_i, S) = h_forward_i + h_backward_i + g'(x_i) as written in Appendix A.2\n",
        "        # AKA A skip connection between inputs and outputs is used\n",
        "        output = forward_output + backward_output + inputs\n",
        "        return output, hn, cn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2br-jufwIY1Z",
        "outputId": "9a89db5a-151f-44b1-8a0c-4ee61c58eb26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 512])\n"
          ]
        }
      ],
      "source": [
        "res = resnet()\n",
        "x = torch.randn((1, 1, 500, 500))\n",
        "x = res(x)\n",
        "print(x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOYOdAonTLAy"
      },
      "outputs": [],
      "source": [
        "class Matching(nn.Module):\n",
        "    def __init__(self, hidden_channels, K, layers, input_channels, fce = False, ):\n",
        "        super(Matching, self).__init__()\n",
        "        self.input_channels = input_channels\n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.K = K\n",
        "        self.layers = layers\n",
        "        self.attLSTM = AttLSTM(self.K, input_size = 512) #hidden size and input size should be same\n",
        "        self.biLSTM = BidirectionalLSTM(512, self.layers)\n",
        "        self.encoder = resnet() #hidden size and input size should be same\n",
        "        self.fce = fce\n",
        "    def forward(self, x, n_shot, k_way, q):\n",
        "        if len(x.shape) ==5: #to make sure that the size of x is 4 dimensional (B, C, H, W)\n",
        "            x = x.squeeze(0).to(device)\n",
        "        embedding_x = self.encoder(x) #embedding of x: (n*k+k*q, feature_dim)\n",
        "        support = embedding_x[:n_shot*k_way]\n",
        "        # print(embedding_x.shape, \"Shape of support \")\n",
        "        query = embedding_x[n_shot*k_way:]\n",
        "        if self.fce:\n",
        "            #in order to feed support set to biLSTM we need to make it 5 dimensional: (L, B, C, H, W)\n",
        "            support = support.unsqueeze(1).to(device)\n",
        "            \n",
        "            g_S, _, _= self.biLSTM(support)\n",
        "            g_S = g_S.squeeze(1)\n",
        "            f_x = self.attLSTM(query, g_S)\n",
        "            distance = torch.cdist(f_x, g_S)\n",
        "            return -distance\n",
        "        else:\n",
        "            return -torch.cdist(query, support) #dimension in this case is \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DV0CQuhgTtos",
        "outputId": "cb3faa99-1fed-422a-d580-c648aecac0ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "CUDA_LAUNCH_BLOCKING=1\n",
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "# del variables\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "model = Matching(hidden_channels = 16, K  = 2, layers = 1, input_channels = 1, fce = True).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def fit_Matching(x, y, n_shot, k_way, q\n",
        ") -> float:\n",
        "    if len(y.shape) == 2:\n",
        "      y = y.squeeze(1)\n",
        "    y_support = y[:n_shot*k_way]\n",
        "    y = y[n_shot*k_way:]\n",
        "    ohe_y_support = F.one_hot(y_support).to(device).float()\n",
        "    optimizer.zero_grad()\n",
        "    scores = model(x, n_shot, k_way, q)\n",
        "    attention = torch.softmax(scores, dim = 1)\n",
        "    # print(attention.shape, \"shape of attention\")\n",
        "    # print(ohe_y_support.shape, \"shape of ohe_y\") #dimension of attention is QxS (Q = k*q), (S = k*n)\n",
        "    prediction = torch.matmul(attention, ohe_y_support)\n",
        "    # print(prediction.shape, \"prediction shape\")\n",
        "    loss = criterion(prediction.log(), y.cuda())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWhiHXkAoUAh",
        "outputId": "61aaa2fe-7ab3-488b-81c6-7cfbd5689d51"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:37<00:00,  2.69it/s, loss=0.61]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.67it/s, loss=0.278]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.67it/s, loss=0.207]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.67it/s, loss=0.309]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.66it/s, loss=0.168]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.67it/s, loss=0.267]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.69it/s, loss=0.119]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.67it/s, loss=0.272]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.69it/s, loss=0.251]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.69it/s, loss=0.202]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.67it/s, loss=0.175]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.69it/s, loss=0.0438]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.67it/s, loss=0.142]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.68it/s, loss=0.0464]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.69it/s, loss=0.0383]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.66it/s, loss=0.0632]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.67it/s, loss=0.0646]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.67it/s, loss=0.0586]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.67it/s, loss=0.11]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.68it/s, loss=0.082]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.68it/s, loss=0.0757]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.67it/s, loss=0.0196]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.67it/s, loss=0.0295]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.67it/s, loss=0.0402]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.68it/s, loss=0.0338]\n"
          ]
        }
      ],
      "source": [
        "CUDA_LAUNCH_BLOCKING=1\n",
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "# del variables\n",
        "gc.collect()\n",
        "\n",
        "log_update_frequency = 10\n",
        "epochs = 25\n",
        "\n",
        "all_loss = []\n",
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "  with tqdm(enumerate(train_dataloader), total=len(train_dataloader)) as tqdm_train:\n",
        "      for episode_index, (x, y) in tqdm_train:\n",
        "          y = y.squeeze(0)\n",
        "          # print(y.shape, \"shape of y to check whether squeeze is true or not\")\n",
        "          # print(y.shape, x.shape, \"X and Y shape\")\n",
        "          # y = \n",
        "          # for i in range(k_way):\n",
        "          #     y[i*n_shot:(i+1)*n_shot] = i\n",
        "          #     y[n_shot*k_way+i*q:n_shot*k_way+(i+1)*q]=i\n",
        "          # # print(y)\n",
        "          y = not_sorted_label(y)\n",
        "          loss_value = fit_Matching(x, y, n_shot, k_way, q)\n",
        "          all_loss.append(loss_value)\n",
        "\n",
        "          if episode_index % log_update_frequency == 0:\n",
        "              tqdm_train.set_postfix(loss=sliding_average(all_loss, log_update_frequency))\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FalQoRA3OFb",
        "outputId": "c851c27a-f0fe-463e-9317-6b3643e74f3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:24<00:00,  4.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model tested on 100 tasks. Accuracy: 94.90%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "evaluate(val_dataloader, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CDOiqbYkU7s"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), '/content/gdrive/My Drive/Matching_ResNet.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-j6k_S4Lk7Mk",
        "outputId": "b0d85cc6-2ea1-42e2-f5af-99eb37c102e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:24<00:00,  4.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model tested on 100 tasks. Accuracy: 95.20%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model2 = Matching(hidden_channels = 16, K  = 2, layers = 1, input_channels = 1, fce = True).to(device)\n",
        "model2.load_state_dict(torch.load('/content/gdrive/My Drive/Matching_ResNet.pth'))\n",
        "model2.fce = False\n",
        "evaluate(val_dataloader, model2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c80VJ1Mj5eWq",
        "outputId": "fbdaf141-e286-4180-c4c8-049dd26ef4d2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:24<00:00,  4.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model tested on 100 tasks. Accuracy: 95.90%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model2.fce = False\n",
        "evaluate(val_dataloader, model2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}