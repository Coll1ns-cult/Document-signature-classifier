{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJT2wPKtb7MU",
        "outputId": "ac25e3d1-df11-443b-f32e-9e8fbc0bc811"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The followings are checkpoints for the models\n",
        "\n",
        "Faster R-CNN: https://drive.google.com/file/d/1WAbR50Eows_lDsevBR8aUV9-RTRKFZ2O/view?usp=sharing\n",
        "\n",
        "Prototypical Network with ResNet Backbone:https://drive.google.com/file/d/1--ZwmUJAvYlCForsYBtLG-Nbi-kelldr/view?usp=sharing\n",
        "\n",
        "Matching Network with ResNet Backbone: https://drive.google.com/file/d/1-6Lxza8lPvnURCJrdG-8MGEEtSeptJeX/view?usp=sharing\n"
      ],
      "metadata": {
        "id": "DG34r98Kh2hD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVuRNROOLdNy"
      },
      "outputs": [],
      "source": [
        "!pip install easyfsl\n",
        "!pip install gradio\n",
        "!pip install git+https://github.com/facebookresearch/detectron2.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gf29o-V0caZk"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import pandas as pd \n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as T\n",
        "from skimage import io\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnet18\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Sampler\n",
        "from torch.nn import Module\n",
        "from easyfsl.utils import plot_images, sliding_average\n",
        "import torch.optim \n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from time import sleep\n",
        "\n",
        "\n",
        "import glob\n",
        "import sys\n",
        "from math import tan, pi\n",
        "import torchvision\n",
        "import cv2\n",
        "\n",
        "\n",
        "\n",
        "import json\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from detectron2.structures import BoxMode\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "\n",
        "\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultTrainer, DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import ColorMode, Visualizer\n",
        "\n",
        "import shutil\n",
        "from PIL import Image\n",
        "from tkinter import Tk\n",
        "from tkinter.filedialog import askdirectory\n",
        "import csv\n",
        "from openpyxl import Workbook\n",
        "\n",
        "#TODO HOOKS\n",
        "from detectron2.engine.hooks import HookBase\n",
        "from detectron2.evaluation import inference_context\n",
        "from detectron2.utils.logger import log_every_n_seconds\n",
        "from detectron2.data import DatasetMapper, build_detection_test_loader\n",
        "import detectron2.utils.comm as comm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "thT1MRsfv3fS"
      },
      "outputs": [],
      "source": [
        "class SignatureDataset(Dataset):\n",
        "    def __init__(self, label_file, root_dir, transform = None):\n",
        "        '''Arguments:\n",
        "        label_file: path to csv file which contains 2 columns:\n",
        "        one with name of the person, other with the label\n",
        "        root_dir: path file of images\n",
        "        transform: transformations that will be applied (default: None)'''\n",
        "        self.df = pd.read_csv(label_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        '''lenght of the file'''\n",
        "        return len(self.df)\n",
        "    def __getitem__(self, index):\n",
        "        '''getting image based on index (protocol in MapDatasets)'''\n",
        "        images = []\n",
        "        labels = []\n",
        "\n",
        "        for ind in index: #since our index coming from \n",
        "            img_path = os.path.join(self.root_dir, str(self.df.iloc[ind, 0]))\n",
        "            image_i = io.imread(img_path)\n",
        "            label_i = torch.tensor(int(self.df.iloc[ind, 1]))\n",
        "\n",
        "            if self.transform:\n",
        "                '''transformations to be done to image'''\n",
        "                image_i = self.transform(image_i)\n",
        "            images.append(image_i)\n",
        "            labels.append(label_i)\n",
        "\n",
        "        #added squeeze to test it\n",
        "        label = torch.stack(labels)\n",
        "        images = torch.stack(images)\n",
        "        return (images, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "BzaYNilTvzoH"
      },
      "outputs": [],
      "source": [
        "class NShotTaskSampler(Sampler):\n",
        "    def __init__(self,\n",
        "                 dataset: torch.utils.data.Dataset,\n",
        "                 episodes_per_epoch: int = None,\n",
        "                 n: int = None,\n",
        "                 k: int = None,\n",
        "                 q: int = None,\n",
        "                 num_tasks: int = 1\n",
        "                #  fixed_tasks: List[Iterable[int]] = None\n",
        "    ):\n",
        "        \"\"\"PyTorch Sampler subclass that generates batches of n-shot, k-way, q-query tasks.\n",
        "\n",
        "        Each n-shot task contains a \"support set\" of `k` sets of `n` samples and a \"query set\" of `k` sets\n",
        "        of `q` samples. The support set and the query set are all grouped into one Tensor such that the first n * k\n",
        "        samples are from the support set while the remaining q * k samples are from the query set.\n",
        "\n",
        "        The support and query sets are sampled such that they are disjoint i.e. do not contain overlapping samples.\n",
        "\n",
        "        # Arguments\n",
        "            dataset: Instance of torch.utils.data.Dataset from which to draw samples\n",
        "            episodes_per_epoch: Arbitrary number of batches of n-shot tasks to generate in one epoch\n",
        "            n_shot: int. Number of samples for each class in the n-shot classification tasks.\n",
        "            k_way: int. Number of classes in the n-shot classification tasks.\n",
        "            q_queries: int. Number query samples for each class in the n-shot classification tasks.\n",
        "            num_tasks: Number of n-shot tasks to group into a single batch\n",
        "            fixed_tasks: If this argument is specified this Sampler will always generate tasks from\n",
        "                the specified classes\n",
        "        \"\"\"\n",
        "        super(NShotTaskSampler, self).__init__(dataset)\n",
        "        self.episodes_per_epoch = episodes_per_epoch\n",
        "        self.dataset = dataset\n",
        "        if num_tasks < 1:\n",
        "            raise ValueError('num_tasks must be > 1.')\n",
        "\n",
        "        self.num_tasks = num_tasks\n",
        "        # TODO: Raise errors if initialise badly\n",
        "        self.k = k\n",
        "        self.n = n\n",
        "        self.q = q\n",
        "        # self.fixed_tasks = fixed_tasks\n",
        "\n",
        "        self.i_task = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.episodes_per_epoch\n",
        "\n",
        "    def __iter__(self):\n",
        "        for _ in range(self.episodes_per_epoch):\n",
        "            batch = []\n",
        "\n",
        "            for task in range(self.num_tasks):\n",
        "                # if self.fixed_tasks is None:\n",
        "                #     # Get random classes\n",
        "                #     episode_classes = np.random.choice(self.dataset.df['class_id'].unique(), size=self.k, replace=False)\n",
        "                # else:\n",
        "                    # Loop through classes in fixed_tasks\n",
        "                # print(self.dataset.df['class_id'].unique())\n",
        "                episode_classes = np.random.choice(self.dataset.df['class_id'].unique(), size=self.k, replace=False)\n",
        "                # print(episode_classes, \"EPISODE CLASSES HERE \")\n",
        "                df = self.dataset.df[self.dataset.df['class_id'].isin(episode_classes)]\n",
        "\n",
        "                support_k = {k: None for k in episode_classes}\n",
        "                for k in episode_classes:\n",
        "                    # Select support examples\n",
        "                    support = df[df['class_id'] == k].sample(self.n)\n",
        "                    # print(support, \"HERE SUPPORT SET\")\n",
        "                    support_k[k] = support\n",
        "\n",
        "                    for i, s in support.iterrows():\n",
        "                        # print(s['id'], \"INSIDE OF SAMPLER\")\n",
        "                        batch.append(s['id'])\n",
        "\n",
        "                # for k in episode_classes:\n",
        "                #     query = df[(df['class_id'] == k) & (~df['id'].isin(support_k[k]['id']))].sample(self.q)\n",
        "                #     for i, q in query.iterrows():\n",
        "                #         # print(q['id'], \"INSIDE OF SAMPLER\")\n",
        "                #         batch.append(q['id'])\n",
        "                query = df[~df['id'].isin(batch)].sample(self.q)\n",
        "                for i, q in query.iterrows():\n",
        "                    batch.append(q['id'])\n",
        "\n",
        "            yield np.stack(batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Please set device"
      ],
      "metadata": {
        "id": "tJPo_BqG0Y0D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ktBjrsP-c9FT"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsBm9Idqc-QQ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yA9Yc5tjcR6l"
      },
      "outputs": [],
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, input_channels, hidden_channels):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.input_channels = input_channels\n",
        "        self.hidden_channels = hidden_channels\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(self.input_channels, self.hidden_channels, kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(self.hidden_channels),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2),\n",
        "            nn.Conv2d(self.hidden_channels, self.hidden_channels, kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(self.hidden_channels),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2),\n",
        "            nn.Conv2d(self.hidden_channels, self.hidden_channels, kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(self.hidden_channels),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2),\n",
        "            nn.Conv2d(self.hidden_channels, self.hidden_channels, kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(self.hidden_channels),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2),\n",
        "            nn.Conv2d(self.hidden_channels, self.hidden_channels, kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(self.hidden_channels),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2),\n",
        "            # nn.AdaptiveMaxPool2d((1,1)),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x)\n",
        "\n",
        "      # return output\n",
        "class resnet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(resnet, self).__init__()\n",
        "        self.resnet_18 = resnet18(pretrained=False)\n",
        "        self.resnet_18.fc = nn.Flatten()\n",
        "    def forward(self, x):\n",
        "        #these steps are required since our images have 1 channel, and resnet-18 takes 3 channel images as input\n",
        "        if x.shape[1] == 3:\n",
        "          return self.resnet_18(x)\n",
        "        elif x.shape[1] == 1:\n",
        "          x = torch.cat((x, x, x), dim = 1)\n",
        "          return self.resnet_18(x)\n",
        "        else:\n",
        "          raise ValueError('shape[1] is not 1 or 3 or it is not even channel dimension')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "t_YKhjr9dkR1"
      },
      "outputs": [],
      "source": [
        "class Proto(nn.Module):\n",
        "    def __init__(self, hidden_channels, input_channels\n",
        "                 #, n_shot, k_way, q\n",
        "                 ):\n",
        "        super(Proto, self).__init__()\n",
        "        # self.n_shot = n_shot\n",
        "        # self.k_way = k_way\n",
        "        # self.q = q\n",
        "        self.input_channels = input_channels\n",
        "        self.hidden_channels = hidden_channels\n",
        "        # self.backbone = ConvNet(self.input_channels, self.hidden_channels)\n",
        "        self.backbone = resnet()\n",
        "    def forward(self, x, n_shot, k_way, q):\n",
        "        #here y_support is already made label:\n",
        "        x = x.squeeze(0).to(device)\n",
        "        embedded_x = self.backbone(x)\n",
        "        # print(embedded_x.shape)\n",
        "        support_set = embedded_x[:k_way*n_shot] #shape of n*k, embedding dim \n",
        "        q_set = embedded_x[k_way*n_shot:] #shape of q*k, embedding dim\n",
        "        mean_support = torch.cat([torch.mean(support_set[i*n_shot:(i+1)*n_shot], dim = 0).unsqueeze(0) for i in range(k_way)]) \n",
        "\n",
        "        l2_distance = torch.cdist(q_set, mean_support) #euclidian distance\n",
        "\n",
        "        return -l2_distance"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Please change n_shot, k_way, q variables as you wish"
      ],
      "metadata": {
        "id": "wlTCCF7u0461"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "SblRQ9llukRd"
      },
      "outputs": [],
      "source": [
        "n_shot = 1\n",
        "k_way = 5\n",
        "q = 10\n",
        "\n",
        "train_set = SignatureDataset('/content/gdrive/My Drive/train_labels.csv', '/content/gdrive/My Drive/full_org', T.Compose([T.ToTensor(), T.Resize((500, 500))]))\n",
        "val_set = SignatureDataset('/content/gdrive/My Drive/val.csv','/content/gdrive/My Drive/full_org_val_1', T.Compose([T.ToTensor(), T.Resize((500, 500))]))\n",
        "test_set = SignatureDataset('/content/gdrive/My Drive/val_labels.csv','/content/gdrive/My Drive/full_org_test', T.Compose([T.ToTensor(), T.Resize((500, 500))]))\n",
        "\n",
        "\n",
        "train_sampler = NShotTaskSampler(dataset = train_set, episodes_per_epoch = 100, n = n_shot, k = k_way, q=q)\n",
        "val_sampler = NShotTaskSampler(dataset = val_set, episodes_per_epoch = 100, n = n_shot, k = k_way, q=q)\n",
        "test_sampler = NShotTaskSampler(dataset = test_set, episodes_per_epoch = 100, n = n_shot, k = k_way, q=q)\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(train_set, sampler = train_sampler) #validation dataloader\n",
        "val_dataloader = DataLoader(val_set, sampler=val_sampler)\n",
        "test_dataloader = DataLoader(test_set, sampler = test_sampler)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "bU-92eK8Zn0D"
      },
      "outputs": [],
      "source": [
        "def not_sorted_label(y:torch.tensor):\n",
        "  if len(y.shape) == 2:\n",
        "    y = y.squeeze(0).to(device)\n",
        "  y_shape = y.shape[0]\n",
        "  # y_clone = y.detach().clone().to(device)\n",
        "  dc = {}\n",
        "  label = 0\n",
        "  for i in range(y_shape):\n",
        "    # print(y[i], \"First time\")\n",
        "    if y[i].item() in dc.keys():\n",
        "      y[i] = dc[y[i].item()]\n",
        "    else:\n",
        "      # print({y[i]:label})\n",
        "      dc.update({y[i].item(): label})\n",
        "      y[i] = label\n",
        "      label += 1\n",
        "    # print(dc)\n",
        "    # print(y[i], \"Second time\")\n",
        "  return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "eGwV2bPGeZdN"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def evaluate_one_episode(x, y, n_shot, q, model):\n",
        "    # print(y.shape, \"here shape of y\")\n",
        "    k_way = (y.shape[0] - q)//n_shot\n",
        "    query_labels = y[n_shot*k_way:]\n",
        "    return (torch.max(\n",
        "            model(x, n_shot, k_way, q).detach().data,1,)[1]\n",
        "        == query_labels.to(device)\n",
        "    ).sum().item(), len(query_labels)\n",
        "\n",
        "def evaluate(data_loader, model):\n",
        "    total_predictions = 0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    model.eval()\n",
        "    \n",
        "    for episode_index, (x, y) in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
        "        y = y.squeeze(0).to(device)\n",
        "        \n",
        "        y = not_sorted_label(y)\n",
        "        correct, total = evaluate_one_episode(\n",
        "            x, y, n_shot, q, model\n",
        "        )\n",
        "\n",
        "        total_predictions += total\n",
        "        correct_predictions += correct\n",
        "\n",
        "    print(\n",
        "        f\"Model tested on {len(data_loader)} tasks. Accuracy: {(100 * correct_predictions/total_predictions):.2f}%\"\n",
        "    )\n",
        "# evaluate(val_dataloader, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjdSg_Q0VZCN"
      },
      "outputs": [],
      "source": [
        "# model = Proto(32, 1\n",
        "#               # , n_shot, k_way, q\n",
        "#               ).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "def fit(x, y, n_shot, q\n",
        ") -> float:\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    k_way = (y.shape[0] - q)//n_shot\n",
        "    y = y[n_shot*k_way:]\n",
        "    scores = model(x, n_shot, k_way, q)\n",
        "    loss = criterion(scores, y.to(device))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZyTvafZcV_e"
      },
      "outputs": [],
      "source": [
        "CUDA_LAUNCH_BLOCKING=1\n",
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "# del variables\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "log_update_frequency = 10\n",
        "epochs = 25\n",
        "\n",
        "all_loss = []\n",
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "  with tqdm(enumerate(train_dataloader), total=len(train_dataloader)) as tqdm_train:\n",
        "      for episode_index, (x, y) in tqdm_train:\n",
        "          y = y.squeeze(0)\n",
        "          y = not_sorted_label(y)\n",
        "          # y = y[n_shot*k_way:]\n",
        "          # print(y)\n",
        "\n",
        "          loss_value = fit(x, y, n_shot, q)\n",
        "          all_loss.append(loss_value)\n",
        "\n",
        "          if episode_index % log_update_frequency == 0:\n",
        "              tqdm_train.set_postfix(loss=sliding_average(all_loss, log_update_frequency))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yn1yxUrj3sq",
        "outputId": "6b8be4d1-488a-40f1-ab28-80c7819aa2ef"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:23<00:00,  4.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model tested on 100 tasks. Accuracy: 98.20%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "evaluate(val_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTWvqTPWTbIb"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), '/content/gdrive/My Drive/Proto_ResNet.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6aFc-IlYYxb",
        "outputId": "326ae114-5434-402f-f2c3-c06a7d575fba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "model = Proto(32, 1\n",
        "              # , n_shot, k_way, q\n",
        "              ).to(device)\n",
        "model.load_state_dict(torch.load('/content/gdrive/My Drive/Proto_ResNet.pth', map_location=device))\n",
        "# evaluate(val_dataloader, model2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(val_dataloader, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDv2cFlgvWzW",
        "outputId": "bd15ad1a-e83d-403f-9c80-29e41c335be4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [05:26<00:00,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model tested on 100 tasks. Accuracy: 96.60%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "705gTe4pauma"
      },
      "outputs": [],
      "source": [
        "# TODO NOT FREEZE\n",
        "def run():\n",
        "    torch.multiprocessing.freeze_support()\n",
        "    print('loop')\n",
        "\n",
        "# TODO GETTING DATA\n",
        "def get_data_dicts(directory, classes):\n",
        "    dataset_dicts = []\n",
        "    for filename in [file for file in os.listdir(directory) if file.endswith('.json')]:\n",
        "        json_file = os.path.join(directory, filename)\n",
        "        with open(json_file) as f:\n",
        "            img_anns = json.load(f)\n",
        "\n",
        "        record = {}\n",
        "\n",
        "        filename = os.path.join(directory, img_anns[\"imagePath\"])\n",
        "\n",
        "        record[\"file_name\"] = filename\n",
        "        record[\"height\"] = 800\n",
        "        record[\"width\"] = 800\n",
        "\n",
        "        annos = img_anns[\"shapes\"]\n",
        "        objs = []\n",
        "        for anno in annos:\n",
        "            px = [a[0] for a in anno['points']]  # x coord\n",
        "            py = [a[1] for a in anno['points']]  # y-coord\n",
        "            poly = [(x, y) for x, y in zip(px, py)]  # poly for segmentation\n",
        "            poly = [p for x in poly for p in x]\n",
        "\n",
        "            obj = {\n",
        "                \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n",
        "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
        "                \"segmentation\": [poly],\n",
        "                \"category_id\": classes.index(anno['label']),\n",
        "                \"iscrowd\": 0\n",
        "            }\n",
        "            objs.append(obj)\n",
        "        record[\"annotations\"] = objs\n",
        "        dataset_dicts.append(record)\n",
        "    return dataset_dicts\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run()\n",
        "\n",
        "\n",
        "# # TODO CLASS AND SHIT\n",
        "classes = ['signature', 'date']\n",
        "\n",
        "data_path = 'C:/Users/kamra/Documents/My Documents/2022 Fall/Deep Learning/Project/ds_reformatted1/'\n",
        "\n",
        "for d in [\"train\", \"test\"]:\n",
        "    DatasetCatalog.register(\n",
        "        \"category_\" + d,\n",
        "        lambda d=d: get_data_dicts(data_path+d, classes)\n",
        "    )\n",
        "    MetadataCatalog.get(\"category_\" + d).set(thing_classes=classes)\n",
        "\n",
        "microcontroller_metadata = MetadataCatalog.get(\"category_train\")\n",
        "\n",
        "\n",
        "# # TODO TRAIN OPTIONS\n",
        "cfg = get_cfg()\n",
        "cfg.MODEL.DEVICE = \"cpu\"\n",
        "# #cfg.MODEL.BACKBONE.NAME = \"build_resnet_backbone\"\n",
        "# #cfg.MODEL.RESNETS.DEPTH = 34\n",
        "# cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\"))\n",
        "# cfg.DATASETS.TRAIN = (\"category_train\",)\n",
        "# cfg.DATASETS.TEST = ()\n",
        "# cfg.TEST.EVAL_PERIOD = 15\n",
        "# cfg.DATALOADER.NUM_WORKERS = 0\n",
        "# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\")\n",
        "# cfg.SOLVER.IMS_PER_BATCH = 8\n",
        "# cfg.SOLVER.BASE_LR = 0.00025\n",
        "# cfg.SOLVER.MAX_ITER = 250\n",
        "# cfg.MODEL.ROI_HEADS.NUM_CLASSES = 2\n",
        "\n",
        "\n",
        "# TODO TEST OPTIONS\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"/content/gdrive/My Drive/model_final.pth\")\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.6\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# TODO TEST ONLY\n",
        "\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def out(imageName):\n",
        "    im = np.array(imageName)\n",
        "    im2 = imageName\n",
        "    outputs = predictor(im)\n",
        "    v = Visualizer(im[:, :, ::-1],\n",
        "                    metadata=microcontroller_metadata)\n",
        "    #\n",
        "    pred_classes = outputs['instances'].pred_classes.cpu().tolist()\n",
        "    class_names = MetadataCatalog.get(\"category_train\").thing_classes\n",
        "    pred_class_names = list(map(lambda x: class_names[x], pred_classes))\n",
        "    #Save detected image\n",
        "    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    #v.save(imageName + \"_test.png\")\n",
        "    # Cropping\n",
        "    i = 0\n",
        "    for pred in range(len(pred_class_names)):\n",
        "        if pred_class_names[pred] == \"signature\":\n",
        "            break\n",
        "        else:\n",
        "            i += 1\n",
        "    boxes = list(outputs[\"instances\"].pred_boxes)\n",
        "    box = boxes[i]\n",
        "    box = box.detach().cpu().numpy()\n",
        "    x_top_left = box[0]\n",
        "    y_top_left = box[1]\n",
        "    x_bottom_right = box[2]\n",
        "    y_bottom_right = box[3]\n",
        "    crop_img = im2.crop((int(x_top_left), int(y_top_left), int(x_bottom_right), int(y_bottom_right)))\n",
        "    # print(int(x_top_left),int(x_bottom_right), int(y_bottom_right), int(y_top_left), \"Results for this\")\n",
        "    # print(im2.shape)\n",
        "    # crop_img = im2[int(x_top_left):int(x_bottom_right), int(y_top_left):int(y_bottom_right)]\n",
        "    #crop_img.save(imageName + \"_cropped.png\")\n",
        "    return v, crop_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ITJKxVl2IpH"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def detection(image):\n",
        "  v, cropped_image = out(image)\n",
        "  v = Image.fromarray(np.uint8(v.get_image())).convert('RGB')\n",
        "  return v, cropped_image\n",
        "\n",
        "\n",
        "demo_detection = gr.Interface(\n",
        "    fn=detection,\n",
        "    inputs=[gr.Image(type=\"pil\")],\n",
        "    outputs =[gr.Image(), gr.Image()]\n",
        ")\n",
        "demo_detection.launch(debug= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzyNtY_TaJ2i"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import requests\n",
        "\n",
        "signatures = [] #signatures to be stores \n",
        "names = [] #carrier of the signatures\n",
        "def label_to_name(names):\n",
        "  return {name:i for i, name in enumerate(names)}\n",
        "\n",
        "transform = T.Compose([T.ToTensor(), T.Grayscale(), T.Resize((200, 200))])\n",
        "def registering_new_signature(name,image):\n",
        "    image = transform(image)\n",
        "    signatures.append(image)\n",
        "    names.append(name)\n",
        "    return str(image.shape[0]) + \",\" + str(image.shape[1]) + \",\" + str(image.shape[2])\n",
        "\n",
        "def classify_signature(image):\n",
        "\n",
        "    v, cropped_image = out(image)\n",
        "    v = Image.fromarray(np.uint8(v.get_image())).convert('RGB')\n",
        "    image = transform(cropped_image)\n",
        "    x = torch.cat((torch.stack(signatures), image.unsqueeze(0)))\n",
        "\n",
        "    probs = torch.softmax(model(x, 1, len(signatures), 1), dim = 1).squeeze(0)\n",
        "\n",
        "\n",
        "    return {str(names[i]):float(probs[i]) for i in range(len(signatures))}, v\n",
        "    \n",
        "demo_register = gr.Interface(\n",
        "    fn=registering_new_signature,\n",
        "    inputs=['text', gr.Image()],\n",
        "    outputs = \"text\"\n",
        ")\n",
        "\n",
        "demo_classifier = gr.Interface(\n",
        "    fn = classify_signature,\n",
        "    inputs = gr.Image(type = 'pil'),\n",
        "    outputs=[gr.Label(), gr.Image()]\n",
        ")\n",
        "\n",
        "demo = gr.TabbedInterface([demo_register, demo_classifier], [\"Registering\", \"Detection & Classification\"])\n",
        "print(names)\n",
        "demo.launch(debug= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMmAGU1Df6iB",
        "outputId": "cde216df-c85d-4a39-c22b-2640507e6b04"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [01:04<00:00,  1.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model tested on 100 tasks. Accuracy: 86.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "evaluate(val_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsb5q3nfQrE7"
      },
      "outputs": [],
      "source": [
        "class AttLSTM(nn.Module):\n",
        "    def __init__(self, K, input_size):\n",
        "        '''attention LSTM with skip connections\n",
        "        Arguments\n",
        "        ---------\n",
        "        K: number of procesing steps\n",
        "        input_size: size of input\n",
        "        hidden_size: size of hidden features'''\n",
        "        super(AttLSTM, self).__init__()\n",
        "        self.processing = K #number of times to run lstm cells ( number of lstm cells basically)\n",
        "        self.input_size = input_size\n",
        "        # self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTMCell(self.input_size, self.input_size)\n",
        "        self.softmax = nn.Softmax(dim = 1)\n",
        "    def forward(self, f_x, g_S):\n",
        "        h = f_x\n",
        "        #g_S: shape (n*k, feature_size)\n",
        "        #f_X: shape: (n*q, feature_size)\n",
        "        c = torch.zeros(f_x.shape[0], f_x.shape[1]).to(device).float() #putting our tensor to device\n",
        "        for _ in range(self.processing):\n",
        "            product = torch.matmul(h, g_S.T)#product shape (nk x nq) -- > (nq x nk)\n",
        "            a = self.softmax(product) #attention\n",
        "            r = torch.matmul(a, g_S) #summation over sequential data\n",
        "            concat = h + r #concatination\n",
        "            h, c  = self.lstm(f_x, (concat, c)) #output of LSTM\n",
        "            h = h + f_x #skip connection\n",
        "        return h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrV7NF8cQrhL"
      },
      "outputs": [],
      "source": [
        "class BidirectionalLSTM(nn.Module):\n",
        "    def __init__(self, size: int, layers: int):\n",
        "        \"\"\"Bidirectional LSTM used to generate fully conditional embeddings (FCE) of the support set as described\n",
        "        in the Matching Networks paper.\n",
        "\n",
        "        # Arguments\n",
        "            size: Size of input and hidden layers. These are constrained to be the same in order to implement the skip\n",
        "                connection described in Appendix A.2\n",
        "            layers: Number of LSTM layers\n",
        "        \"\"\"\n",
        "        super(BidirectionalLSTM, self).__init__()\n",
        "        self.num_layers = layers\n",
        "        self.batch_size = 1\n",
        "        # Force input size and hidden size to be the same in order to implement\n",
        "        # the skip connection as described in Appendix A.1 and A.2 of Matching Networks\n",
        "        self.lstm = nn.LSTM(input_size=size,\n",
        "                            num_layers=layers,\n",
        "                            hidden_size=size,\n",
        "                            bidirectional=True)\n",
        "    def forward(self, inputs):\n",
        "        # Give None as initial state and Pytorch LSTM creates initial hidden states\n",
        "        output, (hn, cn) = self.lstm(inputs, None)\n",
        "\n",
        "        forward_output = output[:, :, :self.lstm.hidden_size]\n",
        "        backward_output = output[:, :, self.lstm.hidden_size:]\n",
        "\n",
        "        # g(x_i, S) = h_forward_i + h_backward_i + g'(x_i) as written in Appendix A.2\n",
        "        # AKA A skip connection between inputs and outputs is used\n",
        "        output = forward_output + backward_output + inputs\n",
        "        return output, hn, cn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOYOdAonTLAy"
      },
      "outputs": [],
      "source": [
        "class Matching(nn.Module):\n",
        "    def __init__(self, hidden_channels, K, layers, input_channels, fce = False, ):\n",
        "        super(Matching, self).__init__()\n",
        "        self.input_channels = input_channels\n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.K = K\n",
        "        self.layers = layers\n",
        "        self.attLSTM = AttLSTM(self.K, input_size = 512) #hidden size and input size should be same\n",
        "        self.biLSTM = BidirectionalLSTM(512, self.layers)\n",
        "        self.encoder = resnet() #hidden size and input size should be same\n",
        "        self.fce = fce\n",
        "    def forward(self, x, n_shot, k_way, q):\n",
        "        if len(x.shape) ==5: #to make sure that the size of x is 4 dimensional (B, C, H, W)\n",
        "            x = x.squeeze(0).to(device)\n",
        "        embedding_x = self.encoder(x) #embedding of x: (n*k+k*q, feature_dim)\n",
        "        support = embedding_x[:n_shot*k_way]\n",
        "        # print(embedding_x.shape, \"Shape of support \")\n",
        "        query = embedding_x[n_shot*k_way:]\n",
        "        if self.fce:\n",
        "            #in order to feed support set to biLSTM we need to make it 5 dimensional: (L, B, C, H, W)\n",
        "            support = support.unsqueeze(1).to(device)\n",
        "            \n",
        "            g_S, _, _= self.biLSTM(support)\n",
        "            g_S = g_S.squeeze(1)\n",
        "            f_x = self.attLSTM(query, g_S)\n",
        "            distance = torch.cdist(f_x, g_S)\n",
        "            return -distance\n",
        "        else:\n",
        "            return -torch.cdist(query, support) #dimension in this case is \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DV0CQuhgTtos",
        "outputId": "cb3faa99-1fed-422a-d580-c648aecac0ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "CUDA_LAUNCH_BLOCKING=1\n",
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "# del variables\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "model = Matching(hidden_channels = 16, K  = 2, layers = 1, input_channels = 1, fce = True).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def fit_Matching(x, y, n_shot, k_way, q\n",
        ") -> float:\n",
        "    if len(y.shape) == 2:\n",
        "      y = y.squeeze(1)\n",
        "    y_support = y[:n_shot*k_way]\n",
        "    y = y[n_shot*k_way:]\n",
        "    ohe_y_support = F.one_hot(y_support).to(device).float()\n",
        "    optimizer.zero_grad()\n",
        "    scores = model(x, n_shot, k_way, q)\n",
        "    attention = torch.softmax(scores, dim = 1)\n",
        "    # print(attention.shape, \"shape of attention\")\n",
        "    # print(ohe_y_support.shape, \"shape of ohe_y\") #dimension of attention is QxS (Q = k*q), (S = k*n)\n",
        "    prediction = torch.matmul(attention, ohe_y_support)\n",
        "    # print(prediction.shape, \"prediction shape\")\n",
        "    loss = criterion(prediction.log(), y.cuda())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWhiHXkAoUAh",
        "outputId": "61aaa2fe-7ab3-488b-81c6-7cfbd5689d51"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:37<00:00,  2.69it/s, loss=0.61]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.67it/s, loss=0.278]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.67it/s, loss=0.207]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.67it/s, loss=0.309]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.66it/s, loss=0.168]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.67it/s, loss=0.267]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.69it/s, loss=0.119]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.67it/s, loss=0.272]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.69it/s, loss=0.251]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.69it/s, loss=0.202]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.67it/s, loss=0.175]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.69it/s, loss=0.0438]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.67it/s, loss=0.142]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.68it/s, loss=0.0464]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.69it/s, loss=0.0383]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.66it/s, loss=0.0632]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.67it/s, loss=0.0646]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.67it/s, loss=0.0586]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.67it/s, loss=0.11]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.68it/s, loss=0.082]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.68it/s, loss=0.0757]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.67it/s, loss=0.0196]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.67it/s, loss=0.0295]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.67it/s, loss=0.0402]\n",
            "100%|██████████| 100/100 [00:37<00:00,  2.68it/s, loss=0.0338]\n"
          ]
        }
      ],
      "source": [
        "CUDA_LAUNCH_BLOCKING=1\n",
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "# del variables\n",
        "gc.collect()\n",
        "\n",
        "log_update_frequency = 10\n",
        "epochs = 25\n",
        "\n",
        "all_loss = []\n",
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "  with tqdm(enumerate(train_dataloader), total=len(train_dataloader)) as tqdm_train:\n",
        "      for episode_index, (x, y) in tqdm_train:\n",
        "          y = y.squeeze(0)\n",
        "          # print(y.shape, \"shape of y to check whether squeeze is true or not\")\n",
        "          # print(y.shape, x.shape, \"X and Y shape\")\n",
        "          # y = \n",
        "          # for i in range(k_way):\n",
        "          #     y[i*n_shot:(i+1)*n_shot] = i\n",
        "          #     y[n_shot*k_way+i*q:n_shot*k_way+(i+1)*q]=i\n",
        "          # # print(y)\n",
        "          y = not_sorted_label(y)\n",
        "          loss_value = fit_Matching(x, y, n_shot, k_way, q)\n",
        "          all_loss.append(loss_value)\n",
        "\n",
        "          if episode_index % log_update_frequency == 0:\n",
        "              tqdm_train.set_postfix(loss=sliding_average(all_loss, log_update_frequency))\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FalQoRA3OFb",
        "outputId": "c851c27a-f0fe-463e-9317-6b3643e74f3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:24<00:00,  4.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model tested on 100 tasks. Accuracy: 94.90%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "evaluate(val_dataloader, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CDOiqbYkU7s"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), '/content/gdrive/My Drive/Matching_ResNet.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-j6k_S4Lk7Mk",
        "outputId": "b0d85cc6-2ea1-42e2-f5af-99eb37c102e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:24<00:00,  4.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model tested on 100 tasks. Accuracy: 95.20%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model2 = Matching(hidden_channels = 16, K  = 2, layers = 1, input_channels = 1, fce = True).to(device)\n",
        "model2.load_state_dict(torch.load('/content/gdrive/My Drive/Matching_ResNet.pth'))\n",
        "model2.fce = False\n",
        "evaluate(val_dataloader, model2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c80VJ1Mj5eWq",
        "outputId": "fbdaf141-e286-4180-c4c8-049dd26ef4d2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:24<00:00,  4.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model tested on 100 tasks. Accuracy: 95.90%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model2.fce = False\n",
        "evaluate(val_dataloader, model2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}